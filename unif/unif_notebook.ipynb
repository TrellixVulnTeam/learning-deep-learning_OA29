{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Francisco UNIF.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG9zxacspJ9i"
   },
   "source": [
    "# UNIF\n",
    "\n",
    "This is Francisco's implemenatation of the UNIF model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpuShs_b4hPs",
    "outputId": "27e966ac-2b25-453b-a978-6665fcd4cb52"
   },
   "source": [
    "# !wget -c https://github.com/ai-center-kth/sentence-cubert/raw/main/model/vocab.txt -P model/"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76zEbrajrt7K",
    "outputId": "52d13e6d-ac1c-4eab-cc58-3b89441a950d"
   },
   "source": [
    "# !pip install transformers\n",
    "# !pip install tokenizer\n",
    "# !pip install tensor2tensor"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tpiazw4pScZ"
   },
   "source": [
    "## Pepare the data\n",
    "\n",
    "The first step to build the model is to prepare the data which is composed of pairs of code snippets and their corresponding natural language descriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C66zhd2irmYG"
   },
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-u3kyOepHJG",
    "outputId": "c28e9559-1959-403b-e932-0cedddbe7599"
   },
   "source": [
    "# !wget -c https://github.com/EdinburghNLP/code-docstring-corpus/raw/master/V2/parallel/parallel_bodies -P ./data/\n",
    "# !wget -c https://github.com/EdinburghNLP/code-docstring-corpus/raw/master/V2/parallel/parallel_desc -P ./data/"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_DsEQp8rp3B"
   },
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usAt3PKYrevU",
    "outputId": "b720c822-50f8-4e67-d65b-f7db4fdc5e01"
   },
   "source": [
    "# DATASET_SIZE = 100000\n",
    "DATASET_SIZE = 10000\n",
    "\n",
    "code_snippets_file = './data/parallel_bodies'\n",
    "with open(code_snippets_file) as f:\n",
    "    code_snippets = [line.rstrip() for line in f]\n",
    "\n",
    "descriptions_file = './data/parallel_desc'\n",
    "# with open(descriptions_file, 'rb') as f:\n",
    "# with open(descriptions_file, encoding=\"utf8\", errors='ignore') as f:\n",
    "with open(descriptions_file, encoding=\"ISO-8859-1\") as f:\n",
    "    descriptions = [line.rstrip() for line in f]\n",
    "\n",
    "print(code_snippets[0])\n",
    "print(descriptions[0])\n",
    "\n",
    "print()\n",
    "print(code_snippets[10])\n",
    "print(descriptions[10])"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DCSP fp = open(filename, 'rb') DCNL DCSP source = (fp.read() + '\\n') DCNL DCSP try: DCNL DCSP  DCSP co = compile(source, filename, 'exec') DCNL DCSP except SyntaxError: DCNL DCSP  DCSP print >>sys.stderr, '>>Syntax DCSP error DCSP in', filename DCNL DCSP  DCSP raise DCNL DCSP fp.close() DCNL DCSP return co\n",
      "'load a Python source file and compile it to byte-code DCNL _load_module(filename: string): code_object DCNL filename:   name of file containing Python source code DCNL (normally a .py) DCNL code_object: code_object compiled from this source code DCNL This function does NOT write any file!'\n",
      "\n",
      " DCSP for body_charset in ('US-ASCII', 'ISO-8859-1', 'UTF-8'): DCNL DCSP  DCSP try: DCNL DCSP  DCSP  DCSP BODY.encode(body_charset) DCNL DCSP  DCSP except UnicodeError: DCNL DCSP  DCSP  DCSP pass DCNL DCSP  DCSP else: DCNL DCSP  DCSP  DCSP break DCNL DCSP msg = MIMEText(BODY.encode(body_charset), 'html', body_charset) DCNL DCSP msg['From'] = SENDER DCNL DCSP msg['To'] = TO DCNL DCSP msg['Subject'] = SUBJECT DCNL DCSP SMTP_PORT = 587 DCNL DCSP session = smtplib.SMTP(SMTP_SERVER, SMTP_PORT) DCNL DCSP session.starttls() DCNL DCSP session.login(FROM, PASSWORD) DCNL DCSP session.sendmail(SENDER, TO, msg.as_string()) DCNL DCSP session.quit()\n",
      "'Sends an HTML email.'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nidkDSbQvAhg"
   },
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "61XN5IxvtJ_u"
   },
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"[CLS]\", 1: \"[SEP]\"}\n",
    "        self.n_words = 2  # Count CLS and SEP\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5RLlHvnt47Y",
    "outputId": "bb69dbde-acd6-48ec-de1a-5ad479eca0d1"
   },
   "source": [
    "code_snippet = \"DCSP fp = open(filename, 'rb') DCNL DCSP source = (fp.read() + '\\n') DCNL DCSP try: DCNL DCSP  DCSP co = compile(source, filename, 'exec') DCNL DCSP except SyntaxError: DCNL DCSP  DCSP print >>sys.stderr, '>>Syntax DCSP error DCSP in', filename DCNL DCSP  DCSP raise DCNL DCSP fp.close() DCNL DCSP return co\"\n",
    "code_tokenizer = Tokenizer()\n",
    "code_tokenizer.add_sentence(code_snippet)\n",
    "code_tokens = code_tokenizer.tokenize(code_snippet)\n",
    "\n",
    "code_snippet_2 = \"DCSP fp DCNL DCSP\"\n",
    "code_tokens_2 = code_tokenizer.tokenize(code_snippet_2)\n",
    "\n",
    "print(code_tokens)\n",
    "print(code_tokens_2)\n",
    "print(code_tokenizer.word2count.keys())"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 2, 8, 4, 9, 10, 11, 7, 2, 12, 7, 2, 13, 2, 14, 4, 15, 16, 17, 7, 2, 18, 19, 7, 2, 13, 2, 20, 21, 22, 2, 23, 2, 24, 25, 7, 2, 13, 2, 26, 7, 2, 27, 7, 2, 28, 14]\n",
      "[2, 3, 7, 2]\n",
      "dict_keys(['DCSP', 'fp', '=', 'open(filename,', \"'rb')\", 'DCNL', 'source', '(fp.read()', '+', \"'\\n')\", 'try:', '', 'co', 'compile(source,', 'filename,', \"'exec')\", 'except', 'SyntaxError:', 'print', '>>sys.stderr,', \"'>>Syntax\", 'error', \"in',\", 'filename', 'raise', 'fp.close()', 'return'])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t-UonqQ8vdai"
   },
   "source": [
    "MODEL_VOCAB = './model/vocab.txt'\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "#\n",
    "# ALL CREDIT GOES TO https://github.com/google-research/google-research/tree/master/cubert\n",
    "#\n",
    "# Copyright 2021 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Cross-language tokenization library.\"\"\"\n",
    "import enum\n",
    "import token as python_token\n",
    "import tokenize\n",
    "from typing import Iterable\n",
    "from typing import List\n",
    "from typing import Mapping\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Text\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "from absl import logging\n",
    "import dataclasses\n",
    "import regex  # Using instead of `re` because it handles Unicode classes.\n",
    "import six\n",
    "\n",
    "\n",
    "# Quote string for special tokens.\n",
    "SPECIAL_QUOTE = '___'\n",
    "\n",
    "\n",
    "def quote_special(content):\n",
    "  return '{q}{t}{q}'.format(q=SPECIAL_QUOTE, t=content)\n",
    "\n",
    "\n",
    "# Log level of pedantic messages.\n",
    "_PEDANTIC = 5\n",
    "\n",
    "# Punctuation for tokenization.\n",
    "SENTINEL = '^'\n",
    "SENTINEL_ESCAPE = 'CARET'\n",
    "\n",
    "\n",
    "@enum.unique\n",
    "class TokenKind(enum.Enum):\n",
    "  \"\"\"The kind of language-agnostic tokens.\"\"\"\n",
    "  NONE = 0  # Sadly, Python2 doesn't support enum.auto()\n",
    "  PUNCTUATION = 1\n",
    "  KEYWORD = 2\n",
    "  IDENTIFIER = 3\n",
    "  STRING = 4\n",
    "  COMMENT = 5\n",
    "  NEWLINE = 6\n",
    "  EOS = 7\n",
    "  ERROR = 8\n",
    "  NUMBER = 9\n",
    "  WHITESPACE = 10\n",
    "\n",
    "\n",
    "NEWLINE = quote_special(TokenKind.NEWLINE.name)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Position():\n",
    "  line: int\n",
    "  column: int\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TokenMetadata():\n",
    "  \"\"\"Metadata about abstract tokens.\n",
    "  Attributes:\n",
    "    start: The position of the first character of the token.\n",
    "    end: The position right after the last character of the token. The line is\n",
    "      the same as the line of the last character and the column is the\n",
    "      column immediately following the last column of the token.\n",
    "  \"\"\"\n",
    "  start: Optional[Position] = None\n",
    "  end: Optional[Position] = None\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class AbstractToken():\n",
    "  spelling: str\n",
    "  kind: TokenKind\n",
    "  metadata: TokenMetadata\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class AbstractMultiToken():\n",
    "  # We force `spellings` to be a concrete `Tuple`, to simplify equality checks\n",
    "  # and hashing. Otherwise, `spellings=[1, 2, 3]` and `spellings=(1, 2, 3)`\n",
    "  # would result in different multi-tokens.\n",
    "  spellings: Tuple[str]\n",
    "  kind: TokenKind\n",
    "  metadata: TokenMetadata\n",
    "\n",
    "\n",
    "def multi_token_from_token(token):\n",
    "  return AbstractMultiToken(spellings=(token.spelling,),\n",
    "                            kind=token.kind,\n",
    "                            metadata=token.metadata)\n",
    "\n",
    "\n",
    "# TODO(maniatis): Add a test for this one, and migrate other copies to use\n",
    "# the same implementation.\n",
    "def fill_range_with_whitespace(start,\n",
    "                               end):\n",
    "  \"\"\"Yields primitive whitespace/newline tokens to fill a text range.\n",
    "  We translate multi-line whitespace into single-line whitespace and newlines,\n",
    "  in a *destructive* canonical fashion. Only space preceding a non-whitespace\n",
    "  token is preserved. Lines with only whitespace are replaced by a single\n",
    "  newline token.\n",
    "  Args:\n",
    "    start: The beginning of the range.\n",
    "    end: The end (exclusive) of the range.\n",
    "  Yields:\n",
    "    WHITESPACE and NEWLINE abstract tokens.\n",
    "  Raises:\n",
    "    ValueError: if `start` does not precede `end`.\n",
    "  \"\"\"\n",
    "  if (start.line, start.column) >= (end.line, end.column):\n",
    "    raise ValueError('`start` must precede `end`, but we received start %s '\n",
    "                     'and end %s.' % (start, end))\n",
    "\n",
    "  current_column = start.column\n",
    "  current_line = start.line\n",
    "  while current_line < end.line:\n",
    "    yield AbstractToken(\n",
    "        quote_special(TokenKind.NEWLINE.name),\n",
    "        TokenKind.NEWLINE,\n",
    "        TokenMetadata(\n",
    "            # A NEWLINE starts at the colum where it occurs and ends\n",
    "            # at the first character of the next line.\n",
    "            start=Position(line=current_line, column=current_column),\n",
    "            end=Position(line=current_line + 1, column=0)))\n",
    "    current_column = 0\n",
    "    current_line += 1\n",
    "\n",
    "  # At this point, we have consumed all newlines. Add any remaining\n",
    "  # space until the next, non-whitespace token.\n",
    "  number_of_final_spaces = end.column - current_column\n",
    "  if number_of_final_spaces:\n",
    "    # Note that we canonicalize all column differences as space characters.\n",
    "    # This, for example, will discard any '\\t' characters and replace them\n",
    "    # with ' '.\n",
    "    yield AbstractToken(\n",
    "        ' ' * number_of_final_spaces, TokenKind.WHITESPACE,\n",
    "        TokenMetadata(\n",
    "            start=Position(line=current_line, column=current_column),\n",
    "            end=Position(line=current_line, column=end.column)))\n",
    "\n",
    "\n",
    "_KINDS_TO_SPLIT_LIKE_WHITESPACE = (TokenKind.COMMENT, TokenKind.STRING,\n",
    "                                   TokenKind.WHITESPACE)\n",
    "_KINDS_TO_SPLIT_BY_LENGTH = (TokenKind.COMMENT, TokenKind.STRING,\n",
    "                             TokenKind.NUMBER, TokenKind.IDENTIFIER,\n",
    "                             TokenKind.WHITESPACE)\n",
    "\n",
    "_UPPERCASE = r'\\p{Lu}'\n",
    "_TITLECASE = r'\\p{Lt}'\n",
    "\n",
    "# Here we abuse the term \"lowercase\", by using it to refer to anything that\n",
    "# doesn't cause a camel/Pascal case split. For Python, for example, this\n",
    "# includes Unicode category Nd (\"decimal numbers\") and Nl (\"number letters\").\n",
    "# We assume that before these regular expressions are applied, any\n",
    "# characters that don't fall into a legal \"other\" category have been checked.\n",
    "_LOWERCASE = r'[^\\p{Lu}\\p{Lt}]'\n",
    "\n",
    "# In what follows, L, U, and T will be replaced with _LOWERCASE, _UPPERCASE\n",
    "# and _TITLECASE later.\n",
    "_CAMEL_AFTER_SNAKE_EXPRESSIONS = [\n",
    "    # Beginning lowercase.\n",
    "    r'^{L}+',\n",
    "    # A single titlecase followed by 0 or more lowercase.\n",
    "    r'{T}{L}*',\n",
    "    # Single uppercase followed by multiple lowercase.\n",
    "    r'{U}{L}+',\n",
    "    # Multiple uppercase ending right before a titlecase.\n",
    "    r'{U}+(?={T})',\n",
    "    # Multiple uppercase ending right before an uppercase followed by lowercase.\n",
    "    r'{U}+(?={U}{L})',\n",
    "    # Multiple uppercase to the end.\n",
    "    r'{U}+$',\n",
    "]\n",
    "_CAMEL_AFTER_SNAKE_EXPRESSION = '|'.join(_CAMEL_AFTER_SNAKE_EXPRESSIONS).format(\n",
    "    L=_LOWERCASE,\n",
    "    T=_TITLECASE,\n",
    "    U=_UPPERCASE)\n",
    "_CAMEL_RE = regex.compile(_CAMEL_AFTER_SNAKE_EXPRESSION, regex.U)  # pytype: disable=module-attr\n",
    "\n",
    "\n",
    "class StateType(enum.IntEnum):\n",
    "  INITIAL_STATE = 0\n",
    "  UPPERCASE_STATE = 1\n",
    "  LOWERCASE_STATE = 2\n",
    "  NUMBER_STATE = 3\n",
    "  SPECIAL_STATE = 4\n",
    "\n",
    "\n",
    "def code_to_tokens(code):\n",
    "  \"\"\"Convert Python source code to list of tokens.\n",
    "  Removes all trailing whitespace and then tokenizes the text as if it were\n",
    "  Python source code. Tokens are 5-tuples as used by the built-in tokenize\n",
    "  module.\n",
    "  Args:\n",
    "    code: string containing python source code\n",
    "  Returns:\n",
    "    The code represented as a string of packed tokens separated by spaces.\n",
    "  Raises:\n",
    "    tokenize.TokenError: When a multi-line token is incomplete. This is\n",
    "      generated by `tokenize.generate_tokens`.\n",
    "    IndentationError: When the source code is incorrectly indented. This is\n",
    "      generated by `tokenize.generate_tokens`.\n",
    "  \"\"\"\n",
    "  token_tuples = list(tokenize.generate_tokens(\n",
    "      six.StringIO(code.rstrip()).readline))\n",
    "  logging.vlog(5, 'Code `%s` was tokenized to token tuples `%s`.', code,\n",
    "               token_tuples)\n",
    "\n",
    "  # Now we get rid of an extraneous trailing newline token, if it has been\n",
    "  # produced. This is a difference in the behavior of generate_tokens between\n",
    "  # Python 2 and Python 3.\n",
    "  if six.PY3:\n",
    "    if len(token_tuples) > 1:\n",
    "      if token_tuples[-2][0] == python_token.NEWLINE:\n",
    "        del token_tuples[-2]\n",
    "        logging.vlog(5, 'Tokenization for `%s` was sanitized. Now token tuples '\n",
    "                     'are `%s`.', code, token_tuples)\n",
    "    # Another similar failure mode is if the final tokens are DEDENT, there may\n",
    "    # be an extraneous newline before them.\n",
    "    if len(token_tuples) > 2:\n",
    "      current = len(token_tuples) - 2  # Right before ENDMARKER.\n",
    "      while current and token_tuples[current][0] == tokenize.DEDENT:\n",
    "        current -= 1\n",
    "      if current and token_tuples[current][0] == tokenize.NEWLINE:\n",
    "        del token_tuples[current]\n",
    "        logging.vlog(5, 'Tokenization for `%s` was sanitized to remove '\n",
    "                     'trailing newline after DEDENTs. Now token tuples are '\n",
    "                     '`%s`.', code, token_tuples)\n",
    "\n",
    "  return token_tuples\n",
    "\n",
    "\n",
    "def code_to_tokens_simple_lossless(code):\n",
    "  r\"\"\"Convert python source code to list of tokens.\n",
    "  This is a simple version using spacing and different classes of characters to\n",
    "  tokenize a string.\n",
    "  A sentence will be split at \"|\" in the following patterns:\n",
    "    upper | upper lower\n",
    "    upper | number\n",
    "    upper | special\n",
    "    lower | upper\n",
    "    lower | number\n",
    "    lower | special\n",
    "    number | upper\n",
    "    number | lower\n",
    "    number | special\n",
    "    special | upper\n",
    "    special | lower\n",
    "    special | number\n",
    "  In addition to splits caused by the type changes above, the code is also split\n",
    "  at whitespace. However, a sequence of spaces or tabs will not be split unless\n",
    "  its length is longer than 20.\n",
    "  For example: \"12345  \\n\\n678\" -> [\"12345\", \"  \", \"\\n\", \"\\n\", \"678\"]\n",
    "  We do not split sequences of spaces/tabs to avoid long sequences of single\n",
    "  \" \" or \"\\t\" tokens caused by deep indentation.\n",
    "  This tokenizer uses a finite state machine. The definition of the states is in\n",
    "  the StateType class.\n",
    "  Args:\n",
    "    code: String containing Python source code.\n",
    "  Returns:\n",
    "    The code represented as a string of tokens separated by spaces.\n",
    "    For example, \"foo  ,1\" -> [\"foo\", \"  \", \",\", \"1\"]\n",
    "  \"\"\"\n",
    "  # normal state transitions that will result in splitting\n",
    "  normal_transitions = [\n",
    "      (StateType.UPPERCASE_STATE, StateType.NUMBER_STATE),\n",
    "      (StateType.UPPERCASE_STATE, StateType.SPECIAL_STATE),\n",
    "      (StateType.LOWERCASE_STATE, StateType.UPPERCASE_STATE),\n",
    "      (StateType.LOWERCASE_STATE, StateType.NUMBER_STATE),\n",
    "      (StateType.LOWERCASE_STATE, StateType.SPECIAL_STATE),\n",
    "      (StateType.NUMBER_STATE, StateType.UPPERCASE_STATE),\n",
    "      (StateType.NUMBER_STATE, StateType.LOWERCASE_STATE),\n",
    "      (StateType.NUMBER_STATE, StateType.SPECIAL_STATE),\n",
    "      (StateType.SPECIAL_STATE, StateType.UPPERCASE_STATE),\n",
    "      (StateType.SPECIAL_STATE, StateType.LOWERCASE_STATE),\n",
    "      (StateType.SPECIAL_STATE, StateType.NUMBER_STATE)]\n",
    "  # output, state\n",
    "  tokens = []\n",
    "  state = StateType.INITIAL_STATE\n",
    "  next_state = None\n",
    "  memory = []\n",
    "  for i, inputchar in enumerate(code):\n",
    "    if inputchar.isupper():\n",
    "      next_state = StateType.UPPERCASE_STATE\n",
    "    elif inputchar.islower():\n",
    "      next_state = StateType.LOWERCASE_STATE\n",
    "    elif inputchar.isdigit():\n",
    "      next_state = StateType.NUMBER_STATE\n",
    "    else:\n",
    "      next_state = StateType.SPECIAL_STATE\n",
    "\n",
    "    # splitting cases\n",
    "    if (state, next_state) in normal_transitions:\n",
    "      tokens.append(''.join(memory))\n",
    "      memory = []\n",
    "    elif (state, next_state) == (StateType.UPPERCASE_STATE,\n",
    "                                 StateType.LOWERCASE_STATE) and len(memory) > 1:\n",
    "      tokens.append(''.join(memory[:-1]))\n",
    "      memory = [memory[-1]]\n",
    "    elif (state, next_state) == (StateType.SPECIAL_STATE,\n",
    "                                 StateType.SPECIAL_STATE):\n",
    "      if inputchar in [' ', '\\t'] and inputchar == code[i-1]:\n",
    "        if len(memory) >= 20:\n",
    "          tokens.append(''.join(memory))\n",
    "          memory = []\n",
    "      elif inputchar.isspace() or code[i-1].isspace():\n",
    "        tokens.append(''.join(memory))\n",
    "        memory = []\n",
    "\n",
    "    # put inputchar into memory, always\n",
    "    memory.append(inputchar)\n",
    "    state = next_state\n",
    "  if memory:\n",
    "    tokens.append(''.join(memory))\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def subtokenize_identifier(identifier):\n",
    "  \"\"\"Splits an identifier assuming camel/pascal/snake case conventions.\n",
    "  This doesn't attempt to classify the identifier as one of snake case/camel/\n",
    "  pascal, etc. It just applies all possible splits in the order snake case,\n",
    "  Pascal, camel.\n",
    "  This doesn't check whether an identifier is a legal identifier for some\n",
    "  language. It is assumed that the caller has already decided that.\n",
    "  For Unicode characters in identifiers, we define splitting conventions as\n",
    "  follows:\n",
    "  - Snake-case is only defined in terms of the ASCII underscore (U+005F). Other\n",
    "    characters that may look like an underscore do not introduce a snake-case\n",
    "    component.\n",
    "  - For the purpose of Pascal and camel cases, we categorize only the Lu Unicode\n",
    "    category as uppercase characters, with the exception of the Lt (titlecase)\n",
    "    character category. Lt characters are treated as a sequence of an uppercase\n",
    "    character followed by a lowercase character and, as such, may only appear\n",
    "    in the beginning of a Pascal-case component, but not as an all-uppercase\n",
    "    component. As an example, if U, L, T are uppercase, lowercase, and titlecase\n",
    "    characters as defined above (i.e., members of Lu, everything else, or Lt\n",
    "    categories, respectively), UUUT would be split as UUU and T, ULTL would be\n",
    "    split as UL and TL, LTL would be split as L and TL, etc.\n",
    "  Args:\n",
    "    identifier: A non-empty string, purporting to be an identifier. Assumes its\n",
    "      validity as an identifier in a given language has already been established\n",
    "      by the caller.\n",
    "  Returns:\n",
    "    A list of substrings of `identifier`. Joining the substrings should return\n",
    "      the original `identifier` exactly.\n",
    "  Raises:\n",
    "    ValueError: if `identifier` is not a legal identifier string.\n",
    "  \"\"\"\n",
    "  snake_splits = identifier.split('_')\n",
    "  snake_components = []  # type: List[Text]\n",
    "  current_snake_separator = []  # type: List[Text]\n",
    "  for snake_split in snake_splits:\n",
    "    if snake_split:\n",
    "      snake_components.append(''.join(current_snake_separator))\n",
    "      current_snake_separator = []\n",
    "      snake_components.append(snake_split)\n",
    "    current_snake_separator.append('_')\n",
    "  # Emit the final separator, but discard the most recent underscore added to\n",
    "  # it. It should have at least one.\n",
    "  current_snake_separator.pop()\n",
    "  if current_snake_separator:\n",
    "    snake_components.append(''.join(current_snake_separator))\n",
    "\n",
    "  # Now we want to do camel-case splitting for each non-underscore snake\n",
    "  # component.\n",
    "  logging.vlog(_PEDANTIC, 'Split %r into snake case: %r', identifier,\n",
    "               snake_components)\n",
    "  all_components = []  # type: List[Text]\n",
    "  for snake_component in snake_components:\n",
    "    if '_' in snake_component:\n",
    "      all_components.append(snake_component)\n",
    "    else:\n",
    "      unicodified_snake_component = six.ensure_text(snake_component)\n",
    "      camel_components = _CAMEL_RE.findall(unicodified_snake_component)\n",
    "      logging.vlog(_PEDANTIC, 'Split snake component %r into %r components.',\n",
    "                   unicodified_snake_component, camel_components)\n",
    "      all_components.extend(camel_components)\n",
    "\n",
    "  # Finally, we want to combine the underscore components with the component\n",
    "  # immediately preceding them.\n",
    "  non_underscore_component = ''\n",
    "  final_components = []  # type: List[Text]\n",
    "  for component in all_components:\n",
    "    if '_' in component:\n",
    "      # Found an underscore component. Combine it with the previous non-\n",
    "      # underscore component (if any), emit it, and clear the remembered\n",
    "      # non-underscore component.\n",
    "      combined_component = non_underscore_component + component\n",
    "      final_components.append(combined_component)\n",
    "      non_underscore_component = ''\n",
    "    else:\n",
    "      # This is a non-underscore component.\n",
    "\n",
    "      if non_underscore_component:\n",
    "        # We've found two consecutive non-underscore components. Emit the\n",
    "        # previous one, since it won't be combined with any underscores.\n",
    "        final_components.append(non_underscore_component)\n",
    "\n",
    "      # Remember the current non-underscore component, in case we need to\n",
    "      # combine it with a following underscore.\n",
    "      non_underscore_component = component\n",
    "  # We may have collected the final non-underscore component and it wasn't\n",
    "  # followed by underscores. Just emit it.\n",
    "  if non_underscore_component:\n",
    "    final_components.append(non_underscore_component)\n",
    "\n",
    "  assert (six.ensure_text(\n",
    "      ''.join(final_components)) == six.ensure_text(identifier)), (\n",
    "          'Ended up with different identifier when joinining components %r '\n",
    "          'into combined %r.' % (final_components, identifier))\n",
    "  return final_components\n",
    "\n",
    "\n",
    "def sanitize(t, mappings):\n",
    "  r\"\"\"Sanitizes a token to remove \"dangerous\" characters, like \\n and \\r.\"\"\"\n",
    "  final = t\n",
    "  for original, sanitized in mappings.items():\n",
    "    assert len(original) == 1\n",
    "    final = final.replace(original, sanitized)\n",
    "  return final\n",
    "\n",
    "\n",
    "def unsanitize(t, mappings):\n",
    "  \"\"\"Unsanitizes a previously sanitized token.\"\"\"\n",
    "  final = t\n",
    "  for original, sanitized in mappings.items():\n",
    "    assert len(original) == 1\n",
    "    final = final.replace(sanitized, original)\n",
    "  return final\n",
    "\n",
    "\n",
    "def split_long_token(token_string,\n",
    "                     max_output_token_length):\n",
    "  \"\"\"Splits a token losslessly to some maximum length per component.\n",
    "  A long token is split into multiple tokens. For instance, `'bcd'` with\n",
    "  `max_output_token_length=2` will become `['bc', 'd']`. No sentinel or other\n",
    "  split mark is added at this stage.\n",
    "  A token is assumed to be non-empty.\n",
    "  Args:\n",
    "    token_string: The token.\n",
    "    max_output_token_length: Maximum length of an output token.\n",
    "  Returns:\n",
    "    List of split tokens.\n",
    "  Raises:\n",
    "    ValueError: if `token` is empty.\n",
    "  \"\"\"\n",
    "  if not token_string:\n",
    "    raise ValueError('Expected %r to be non-empty' % token_string)\n",
    "\n",
    "  whole_token_length = len(token_string)\n",
    "  remainder_length = whole_token_length % max_output_token_length\n",
    "  even_parts = list(\n",
    "      map(\n",
    "          # ...join together...\n",
    "          ''.join,\n",
    "          zip(\n",
    "              # `max_output_token_length` copies of the iterator of\n",
    "              # whole_token's characters. zip will draw from the same iterator\n",
    "              # and return `max_output_token_length` tuples of characters from\n",
    "              # `whole_token`.\n",
    "              *[iter(token_string)] * max_output_token_length)))\n",
    "  remainder_part = ([token_string[-remainder_length:]]\n",
    "                    if remainder_length else [])\n",
    "  split_token = even_parts + remainder_part\n",
    "  assert split_token, ('while wrapping >>%s<< into >%r<' %\n",
    "                       (token_string, split_token))\n",
    "  assert all([\n",
    "      len(t) <= max_output_token_length for t in split_token\n",
    "  ]), ('Got split_token >>>%r<<<, which contains tokens longer than %d.' %\n",
    "       (split_token, max_output_token_length))\n",
    "  return split_token\n",
    "\n",
    "\n",
    "def _agnostic_tokens_to_lists_of_token_lists(\n",
    "    agnostic_tokens\n",
    "):\n",
    "  \"\"\"Turns each token into a singleton token list, keeping token kinds.\"\"\"\n",
    "  return [multi_token_from_token(a) for a in agnostic_tokens]\n",
    "\n",
    "\n",
    "def _subtokenize_identifiers_heuristically(\n",
    "    token_lists\n",
    "):\n",
    "  \"\"\"Subtokenizes only identifiers in a list of token lists.\n",
    "  This assumes that every subtoken list is still a singleton.\n",
    "  Args:\n",
    "    token_lists: A list of labelled tokens. Each token is represented as a\n",
    "      (still) singleton list of subtokens.\n",
    "  Returns:\n",
    "    A list of token lists, of which the identifiers are split heuristically.\n",
    "  \"\"\"\n",
    "  with_split_identifiers: List[AbstractMultiToken] = []\n",
    "  for multi_token in token_lists:\n",
    "    # spelling_list had better still be a singleton.\n",
    "    assert len(multi_token.spellings) == 1, (\n",
    "        'Expected %r to be a singleton, but it is not.' % multi_token)\n",
    "    if multi_token.kind is TokenKind.IDENTIFIER:\n",
    "      subtokenized = dataclasses.replace(\n",
    "          multi_token,\n",
    "          spellings=subtokenize_identifier(multi_token.spellings[0]))\n",
    "      with_split_identifiers.append(subtokenized)\n",
    "    else:\n",
    "      with_split_identifiers.append(multi_token)\n",
    "  return with_split_identifiers\n",
    "\n",
    "\n",
    "def _subtokenize_strings_heuristically(\n",
    "    token_lists\n",
    "):\n",
    "  \"\"\"Splits STRING, COMMENT, WHITESPACE tokens like text.\n",
    "  Args:\n",
    "    token_lists: List of subtoken lists, of which only those of kind IDENTIFIER\n",
    "      are allowed not to be singletons.\n",
    "  Returns:\n",
    "    A list of token lists, of which IDENTIFIER, STRING, NUMBER, COMMENT tokens\n",
    "      are now split heuristically.\n",
    "  \"\"\"\n",
    "  with_heuristically_split_text: List[AbstractMultiToken] = []\n",
    "  for multi_token in token_lists:\n",
    "    if multi_token.kind in _KINDS_TO_SPLIT_LIKE_WHITESPACE:\n",
    "      assert len(multi_token.spellings) == 1, (\n",
    "          'Expected %r to be a singleton, but it is not.' % multi_token)\n",
    "      subtokenized = dataclasses.replace(\n",
    "          multi_token,\n",
    "          spellings=code_to_tokens_simple_lossless(multi_token.spellings[0]))\n",
    "      with_heuristically_split_text.append(subtokenized)\n",
    "    else:\n",
    "      with_heuristically_split_text.append(multi_token)\n",
    "  return with_heuristically_split_text\n",
    "\n",
    "\n",
    "def _shorten_subtokens(\n",
    "    token_lists,\n",
    "    max_output_token_length,\n",
    "):\n",
    "  \"\"\"Further subtokenizes any subtokens that are too long.\n",
    "  At this point, we're done with all heuristic splitting. Now split what's left\n",
    "  by length if need be. We don't do anything about keywords or other\n",
    "  punctuation.\n",
    "  Args:\n",
    "    token_lists: List of subtoken lists, of which only those of kinds\n",
    "      IDENTIFIER, NUMBER, STRING, COMMENT may have been subtokenized.\n",
    "    max_output_token_length: The max character length for each subtoken of\n",
    "      the subtokenizable kinds.\n",
    "  Returns:\n",
    "    Subtokenized tokens up to a maximum per-subtoken length.\n",
    "  \"\"\"\n",
    "  shortened_subtokens: List[AbstractMultiToken] = []\n",
    "  for multi_token in token_lists:\n",
    "    if multi_token.kind in _KINDS_TO_SPLIT_BY_LENGTH:\n",
    "      shortened_spelling_list: List[str] = []\n",
    "      for spelling in multi_token.spellings:\n",
    "        shortened_spelling_list.extend(\n",
    "            split_long_token(spelling, max_output_token_length))\n",
    "      shortened_subtokens.append(\n",
    "          dataclasses.replace(\n",
    "              multi_token, spellings=tuple(shortened_spelling_list)))\n",
    "    else:\n",
    "      shortened_subtokens.append(multi_token)\n",
    "  return shortened_subtokens\n",
    "\n",
    "\n",
    "def split_agnostic_tokens(\n",
    "    agnostic_tokens,\n",
    "    max_output_token_length,\n",
    "):\n",
    "  \"\"\"Splits each language-agnostic token according to its kind.\n",
    "  Args:\n",
    "    agnostic_tokens: The language-agnostic tokens to subtokenize. These are\n",
    "      pairs of spelling and generic token kind. No subtokenization has been\n",
    "      done; the tokens are as the language-specific lexer produced them.\n",
    "    max_output_token_length: The target maximum output token length.\n",
    "  Returns:\n",
    "    A list of subtoken lists, with their associated token kind.\n",
    "  \"\"\"\n",
    "  # Prepare for subtokenization.\n",
    "  agnostic_token_lists = _agnostic_tokens_to_lists_of_token_lists(\n",
    "      agnostic_tokens)\n",
    "  # Perform heuristic subtokenizations.\n",
    "  with_identifiers_heuristically_split = _subtokenize_identifiers_heuristically(\n",
    "      agnostic_token_lists)\n",
    "  with_string_tokens_heuristically_split = _subtokenize_strings_heuristically(\n",
    "      with_identifiers_heuristically_split)\n",
    "  # Shorten resulting subtokens by length.\n",
    "  shortened_subtokens = _shorten_subtokens(\n",
    "      with_string_tokens_heuristically_split, max_output_token_length)\n",
    "\n",
    "  return shortened_subtokens\n",
    "\n",
    "\n",
    "def sanitize_subtoken_lists(\n",
    "    subtoken_lists,\n",
    "    sanitization_mapping,\n",
    "    sentinel):\n",
    "  \"\"\"Sanitizes lists of subtoken lists, adding sentinels.\n",
    "  Args:\n",
    "    subtoken_lists: A list of multi-tokens. Cannot be empty or contain empty\n",
    "      sublists.\n",
    "    sanitization_mapping: A mapping from sensitive characters to replacement\n",
    "      strings. It is assumed to have been checked by `check_mappings`.\n",
    "    sentinel: The sentinel character. It is expected to be one of the keys\n",
    "      in `sanitization_mapping`.\n",
    "  Returns:\n",
    "    A list of multi-tokens.\n",
    "  Raises:\n",
    "    ValueError: If one of the input sublists is empty, or the entire input\n",
    "      is empty, or the sentinel is not one of the unsanitary characters.\n",
    "  \"\"\"\n",
    "  if not subtoken_lists:\n",
    "    raise ValueError('Received empty input %r but expected it to be non '\n",
    "                     'empty' % subtoken_lists)\n",
    "  if sentinel not in sanitization_mapping:\n",
    "    raise ValueError('Sentinel %r should be in the sanitization map %r '\n",
    "                     'but is not.' % (sentinel, sanitization_mapping))\n",
    "\n",
    "  sanitized_lists = []\n",
    "  for multi_token in subtoken_lists:\n",
    "    spellings = multi_token.spellings\n",
    "    if not spellings:\n",
    "      raise ValueError('Received empty multi-token %r but expected no empty '\n",
    "                       'ones' % multi_token)\n",
    "    sanitized_spellings = [\n",
    "        sanitize(t, sanitization_mapping)\n",
    "        for t in spellings\n",
    "    ]\n",
    "\n",
    "    # Add the sentinel to all subtokens except the last one.\n",
    "    with_sentinel = ([t + sentinel for t in sanitized_spellings[:-1]] +\n",
    "                     [sanitized_spellings[-1]])\n",
    "\n",
    "    sanitized_lists.append(\n",
    "        dataclasses.replace(multi_token, spellings=with_sentinel))\n",
    "  return sanitized_lists\n",
    "\n",
    "\n",
    "def flatten_subtoken_lists(\n",
    "    subtoken_lists):\n",
    "  \"\"\"Flattens lists of subtoken lists.\n",
    "  Args:\n",
    "    subtoken_lists: A list of subtoken lists, one list per initial language\n",
    "      token. Cannot be empty or contain empty sublits.\n",
    "  Returns:\n",
    "    A list of flattened subtokens representing the entire original sequence.\n",
    "  Raises:\n",
    "    ValueError: If the input is empty.\n",
    "  \"\"\"\n",
    "  if not subtoken_lists:\n",
    "    raise ValueError('Received empty input %r but expected it to be non '\n",
    "                     'empty' % (subtoken_lists,))\n",
    "  spellings = (t.spellings for t in subtoken_lists)\n",
    "  subtokens = sum(spellings, [])\n",
    "\n",
    "  return subtokens\n",
    "\n",
    "\n",
    "def flatten_and_sanitize_subtoken_lists(\n",
    "    subtoken_lists,\n",
    "    sanitization_mapping,\n",
    "    sentinel):\n",
    "  \"\"\"Sanitizes and then flattens lists of subtoken lists, adding sentinels.\n",
    "  Args:\n",
    "    subtoken_lists: A list of multi-tokens, one per initial language\n",
    "      token. Cannot be empty or contain empty sublits.\n",
    "    sanitization_mapping: A mapping from sensitive characters to replacement\n",
    "      strings. It is assumed to have been checked by `check_mappings`.\n",
    "    sentinel: The sentinel character. It is expected to be one of the keys\n",
    "      in `sanitization_mapping`.\n",
    "  Returns:\n",
    "    A list of flattened subtokens representing the entire original sequence.\n",
    "  Raises:\n",
    "    ValueError: If one of the input sublists is empty, or the entire input\n",
    "      is empty, or the sentinel is not one of the unsanitary characters.\n",
    "  \"\"\"\n",
    "  sanitized = sanitize_subtoken_lists(subtoken_lists, sanitization_mapping,\n",
    "                                      sentinel)\n",
    "  flattened = flatten_subtoken_lists(sanitized)\n",
    "  return flattened\n",
    "\n",
    "\n",
    "def reconstitute_full_unsanitary_tokens(\n",
    "    split_token_list,\n",
    "    sanitization_mapping,\n",
    "    sentinel):\n",
    "  \"\"\"Unsplits tokens previously subtokenized and flattened.\n",
    "  It assumes this is the output of `split_agnostic_tokens`, followed by\n",
    "  `sanitize_subtoken_lists` and `flatten_subtoken_lists`.\n",
    "  Split tokens are joined together.  `['bc^', 'd']` will become\n",
    "  `'bcd'`, where '^' is `SENTINEL` that indicates where joining occurs.\n",
    "  Args:\n",
    "    split_token_list: List of split tokens.\n",
    "    sanitization_mapping: A mapping from sensitive characters to replacement\n",
    "      strings. It is assumed to have been checked by `check_mappings`.\n",
    "    sentinel: The sentinel character. It is expected to be one of the keys\n",
    "      in `sanitization_mapping`.\n",
    "  Returns:\n",
    "    Sequence of whole tokens.\n",
    "  Raises:\n",
    "    ValueError: if the sentinel character appears in any position other than\n",
    "      the sentinel position, or if any of the unsanitary characters (as per\n",
    "      the `sanitization_mapping`) appear anywhere, or if a subtoken is empty,\n",
    "      or the sentinel is not one of the unsanitary characters.\n",
    "  \"\"\"\n",
    "  if not split_token_list:\n",
    "    raise ValueError('Received empty input %r but expected it to be non '\n",
    "                     'empty.' % split_token_list)\n",
    "  if sentinel not in sanitization_mapping:\n",
    "    raise ValueError('Sentinel %r should be in the sanitization map %r '\n",
    "                     'but is not.' % (sentinel, sanitization_mapping))\n",
    "\n",
    "  whole_token_list = []  # type: List[Text]\n",
    "  pending_split_tokens = []  # type: List[Text]\n",
    "  for t in split_token_list:\n",
    "    if not t:\n",
    "      raise ValueError('Must have non-empty subtokens, but found %r in %r.' % (\n",
    "          t, split_token_list))\n",
    "    if t[-1] == sentinel:\n",
    "      # Remove sentinel and accumulate until the final one appears.\n",
    "      pending_split_tokens.append(t[:-1])\n",
    "    else:\n",
    "      # It is a final token, so combine everything accumulated into one.\n",
    "      pending_split_tokens.append(t)\n",
    "      whole_token = ''.join(pending_split_tokens)\n",
    "      whole_token_list.append(whole_token)\n",
    "      pending_split_tokens = []\n",
    "  # We should have nothing pending.\n",
    "  if pending_split_tokens:\n",
    "    raise ValueError('After scanning all subtokens %r, there still is some '\n",
    "                     'unjoined content: %r' %\n",
    "                     (split_token_list, pending_split_tokens))\n",
    "\n",
    "  # At this point we have whole tokens that contain sanitized characters. First\n",
    "  # we'll see if they are dirty, and then unsanitize them into their original\n",
    "  # form.\n",
    "  unsanitary_characters = sanitization_mapping.keys()\n",
    "  for whole_token in whole_token_list:\n",
    "    for unsanitary_character in unsanitary_characters:\n",
    "      if unsanitary_character in whole_token:\n",
    "        raise ValueError('Reconstructed whole token %r seems to contain a '\n",
    "                         'character %r that should have been sanitized '\n",
    "                         'already.' % (whole_token, unsanitary_character))\n",
    "  # Unsanitize.\n",
    "  unsanitized_whole_tokens = [\n",
    "      unsanitize(t, sanitization_mapping) for t in whole_token_list\n",
    "  ]\n",
    "\n",
    "  return unsanitized_whole_tokens\n",
    "\n",
    "\n",
    "def check_mappings(mappings):\n",
    "  \"\"\"Checks the correctness of character-to-string sanitization mappings.\n",
    "  This ensures that all keys are single characters and that no value contains\n",
    "  any of the keys or other values.\n",
    "  Args:\n",
    "    mappings: A mapping from characters to strings.\n",
    "  Raises:\n",
    "    ValueError: If a key has length different from 1 or if a key appears in any\n",
    "      value or if a value is a substring of another value, or if any value is\n",
    "      empty or non-unique.\n",
    "  \"\"\"\n",
    "  for key in mappings:\n",
    "    if len(key) != 1:\n",
    "      raise ValueError('Expecting length-1 strings as keys in mappings, but '\n",
    "                       'got key %r in mappings %r.' % (key, mappings))\n",
    "\n",
    "  values = mappings.values()\n",
    "\n",
    "  if len(values) != len(set(values)):\n",
    "    raise ValueError('There seem to be some duplicate values in %r, but they '\n",
    "                     'are expected to be unique.' % mappings)\n",
    "\n",
    "  if any([not value for value in values]):\n",
    "    raise ValueError('An empty value found in %r, but no empty values are '\n",
    "                     'allowed.' % mappings)\n",
    "\n",
    "  for value in values:\n",
    "    for other_value in values:\n",
    "      if value != other_value and value in other_value:\n",
    "        raise ValueError('Value %r is a substring of %r, but no value may '\n",
    "                         'be a substring of another.' % (value, other_value))\n",
    "\n",
    "    for key in mappings:\n",
    "      if key in value:\n",
    "        raise ValueError('No key may appear in one of the mapping values, but '\n",
    "                         'found key %r in value %r, both of which appear in '\n",
    "                         'the mappings %r.' % (key, value, mappings))\n",
    "\n",
    "\n",
    "def subtokenize_agnostic_tokens_in_place(\n",
    "    agnostic_tokens,\n",
    "    max_output_token_length,\n",
    "    sanitization_mapping,\n",
    "    sentinel,\n",
    "):\n",
    "  \"\"\"Subtokenizes language-agnostic tokens, discarding their kind in the end.\n",
    "  Args:\n",
    "    agnostic_tokens: The language-agnostic tokens to subtokenize. These are\n",
    "      pairs of spelling and generic token kind. No subtokenization has been\n",
    "      done; the tokens are as the language-specific lexer produced them.\n",
    "    max_output_token_length: The target maximum output token length.\n",
    "    sanitization_mapping: A mapping from sensitive characters to replacement\n",
    "      strings. It is assumed to have been checked by `check_mappings`.\n",
    "    sentinel: The sentinel character. It is expected to be one of the keys\n",
    "      in `sanitization_mapping`.\n",
    "  Returns:\n",
    "    A list of subtoken lists, one per original agnostic token.\n",
    "  \"\"\"\n",
    "  labelled_subtokenized = split_agnostic_tokens(agnostic_tokens,\n",
    "                                                max_output_token_length)\n",
    "\n",
    "  subtoken_lists = sanitize_subtoken_lists(labelled_subtokenized,\n",
    "                                           sanitization_mapping,\n",
    "                                           sentinel)\n",
    "  return subtoken_lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "#\n",
    "# ALL CREDIT GOES TO https://github.com/google-research/google-research/tree/master/cubert\n",
    "#\n",
    "# Copyright 2021 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"This module contains utilities for source code tokenization.\"\"\"\n",
    "\n",
    "import abc\n",
    "import tokenize\n",
    "from typing import Collection\n",
    "from typing import Dict\n",
    "from typing import Iterable\n",
    "from typing import Mapping\n",
    "from typing import Sequence\n",
    "from typing import Text\n",
    "from typing import Union\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "# from tokenizer import unified_tokenizer\n",
    "\n",
    "# After all splitting, the longest a token is of the following length.\n",
    "MAX_OUTPUT_TOKEN_LENGTH = 15\n",
    "\n",
    "\n",
    "class CuBertTokenizer(abc.ABC):\n",
    "    \"\"\"A tokenizer that implements a language-agnostic tokenization.\n",
    "    The tokenizer implements a language-agnostic tokenization. This is available\n",
    "    as `tokenize_and_abstract()`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_output_token_length=MAX_OUTPUT_TOKEN_LENGTH,\n",
    "                 reserved=()):\n",
    "        self.types_to_skip = ()\n",
    "        self.reserved = reserved\n",
    "        self.mappings: Dict[str, str]\n",
    "        self.update_mappings({\n",
    "            # By default, replace \\n and \\r. This is meant primarily for literals.\n",
    "            '\\n':\n",
    "                quote_special('NLCHAR'),\n",
    "            '\\r':\n",
    "                quote_special('CR'),\n",
    "            SENTINEL:\n",
    "                quote_special(\n",
    "                    SENTINEL_ESCAPE),\n",
    "        })\n",
    "        self.max_output_token_length = max_output_token_length\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def tokenize_and_abstract(\n",
    "            self,\n",
    "            source_code):\n",
    "        \"\"\"Produces a language-agnostic tokenization of the input code.\n",
    "        Args:\n",
    "          source_code: Source code stored in a string.\n",
    "        Returns:\n",
    "          A list of pairs of a token (string) and a token kind in the given source\n",
    "            code. It always includes an end of sequence token. That is, an empty\n",
    "            input always returns a list of size 1.\n",
    "        Raises:\n",
    "          ValueError: if `source_code` cannot be tokenized.\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def untokenize_abstract(self, whole_tokens):\n",
    "        \"\"\"Applies language-specific rules to an abstract untokenized list.\n",
    "        Args:\n",
    "          whole_tokens: Abstract tokens, reconstituted and unsanitized by\n",
    "            `untokenize` before passed to this language-specific logic.\n",
    "        Returns:\n",
    "          A string representing the untokenized text.\n",
    "        \"\"\"\n",
    "\n",
    "    def update_types_to_skip(\n",
    "        self, types_to_skip\n",
    "    ):\n",
    "        \"\"\"Replaces the set of token types that are ignored.\n",
    "        Each tokenizer may provide different semantics with respect to this list,\n",
    "        and may ignore it altogether.\n",
    "        Args:\n",
    "          types_to_skip: Types (from the constants in the `token` module) or\n",
    "            `unified_tokenizer.TokenKind`. Note that some of those constants are\n",
    "            actually defined in the `tokenize` module.\n",
    "        \"\"\"\n",
    "        self.types_to_skip = types_to_skip\n",
    "\n",
    "    def replace_reserved_keywords(self, reserved):\n",
    "        \"\"\"Replaces the reserved keywords with the supplied list of strings.\n",
    "        Each tokenizer may provide different semantics with respect to the list\n",
    "        of reserved keywords, or ignore them altogether.\n",
    "        Args:\n",
    "          reserved: List of strings.\n",
    "        \"\"\"\n",
    "        self.reserved = reserved  # Replace the old one entirely.\n",
    "\n",
    "    def update_mappings(self, mappings):\n",
    "        \"\"\"Replaces the character mappings with the supplied dictionary.\n",
    "        The intent for character mappings is to enable tokenizers that support them\n",
    "        to sanitize dangerous characters, such as newline and carriage return,\n",
    "        with a nicer symbol.\n",
    "        Each tokenizer may provide different semantics with respect to the\n",
    "        mappings, or ignore them altogether.\n",
    "        Args:\n",
    "          mappings: Dictionary of original to sanitized strings. Keys are expected\n",
    "            to have length 1.\n",
    "        Raises:\n",
    "          ValueError: if a key has length different from 1.\n",
    "        \"\"\"\n",
    "        check_mappings(mappings)\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def get_mappings(self):\n",
    "        return self.mappings\n",
    "\n",
    "    def condition_full_tokens(\n",
    "        self, agnostic\n",
    "    ):\n",
    "        \"\"\"Applies reserved keywords and character sanitization.\"\"\"\n",
    "        filtered: Iterable[AbstractToken] = (\n",
    "            a for a in agnostic if a.kind not in self.types_to_skip)\n",
    "\n",
    "        # Now turn all reserved words, regardless of kind, into keywords.\n",
    "        with_reserved: Sequence[AbstractToken] = tuple(\n",
    "            dataclasses.replace(a, kind=TokenKind.KEYWORD)\n",
    "            if a.spelling in self.reserved else a\n",
    "            for a in filtered)\n",
    "        return with_reserved\n",
    "\n",
    "    def subtokenize_full_tokens(\n",
    "        self, agnostic\n",
    "    ):\n",
    "        \"\"\"Performs heuristic splitting of full tokens.\"\"\"\n",
    "        subtoken_lists = subtokenize_agnostic_tokens_in_place(\n",
    "            agnostic_tokens=agnostic,\n",
    "            max_output_token_length=self.max_output_token_length,\n",
    "            sanitization_mapping=self.mappings,\n",
    "            sentinel=SENTINEL)\n",
    "        return subtoken_lists\n",
    "\n",
    "    def tokenize(self, source_code):\n",
    "        \"\"\"Tokenizes via `tokenize_and_abstract`.\"\"\"\n",
    "        try:\n",
    "            agnostic = self.tokenize_and_abstract(source_code)\n",
    "        except Exception as e:\n",
    "            raise ValueError('While trying to do language-specific tokenization for '\n",
    "                             'the string:\\n\\n\\n%r\\n\\n\\n%s\\n\\n\\n'\n",
    "                             'we received error %r.' % (source_code, source_code, e))\n",
    "\n",
    "        conditioned = self.condition_full_tokens(agnostic)\n",
    "\n",
    "        multi_tokens = self.subtokenize_full_tokens(conditioned)\n",
    "\n",
    "        subtokens = flatten_subtoken_lists(multi_tokens)\n",
    "        return subtokens\n",
    "\n",
    "    def untokenize(self, token_list):\n",
    "        \"\"\"Untokenizes via `untokenize_abstract`.\"\"\"\n",
    "        # Untokenize agnostic.\n",
    "        if (not token_list or token_list[-1] != quote_special(\n",
    "                TokenKind.EOS.name)):\n",
    "            raise ValueError('Token list %r should end with the EOS token %r.' %\n",
    "                             (token_list,\n",
    "                              quote_special(\n",
    "                                  TokenKind.EOS.name)))\n",
    "\n",
    "        whole_tokens = reconstitute_full_unsanitary_tokens(\n",
    "            token_list,\n",
    "            sanitization_mapping=self.mappings,\n",
    "            sentinel=SENTINEL)\n",
    "\n",
    "        return self.untokenize_abstract(whole_tokens)\n",
    "\n",
    "\n",
    "def token_from_token_type(token_type):\n",
    "    \"\"\"Turns a token type into a reserved token string.\"\"\"\n",
    "    # We use the tok_name dict from tokenize, not token. The former has\n",
    "    # NL and COMMENT and such, whereas the latter doesn't.\n",
    "    return quote_special(tokenize.tok_name[token_type])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "#\n",
    "# ALL CREDIT GOES TO https://github.com/google-research/google-research/tree/master/cubert\n",
    "#\n",
    "# Copyright 2021 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"A Python tokenizer subclass of CuBertTokenizer.\"\"\"\n",
    "import keyword\n",
    "import re\n",
    "import tokenize\n",
    "import typing\n",
    "from typing import Any\n",
    "from typing import List\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from absl import logging\n",
    "# from . import cubert_tokenizer\n",
    "# from . import unified_tokenizer\n",
    "\n",
    "\n",
    "class PythonTokenizer(CuBertTokenizer):\n",
    "    \"\"\"Tokenizer that extracts Python's lexical elements preserving strings.\"\"\"\n",
    "    _TOKEN_TYPE_MAP = {\n",
    "        tokenize.COMMENT:    TokenKind.COMMENT,\n",
    "        tokenize.DEDENT:     TokenKind.KEYWORD,\n",
    "        tokenize.ENDMARKER:  TokenKind.EOS,\n",
    "        tokenize.ERRORTOKEN: TokenKind.ERROR,\n",
    "        tokenize.INDENT:     TokenKind.KEYWORD,\n",
    "        tokenize.NEWLINE:    TokenKind.NEWLINE,\n",
    "        tokenize.NL:         TokenKind.PUNCTUATION,\n",
    "        tokenize.NUMBER:     TokenKind.NUMBER,\n",
    "        tokenize.OP:         TokenKind.PUNCTUATION,\n",
    "        tokenize.STRING:     TokenKind.STRING,\n",
    "    }\n",
    "    _REVERSE_TOKEN_MAP = {\n",
    "        token_from_token_type(tokenize.INDENT):\n",
    "            tokenize.INDENT,\n",
    "        token_from_token_type(tokenize.DEDENT):\n",
    "            tokenize.DEDENT,\n",
    "        quote_special(TokenKind.EOS.name):\n",
    "            tokenize.ENDMARKER,\n",
    "        quote_special(TokenKind.ERROR.name):\n",
    "            tokenize.ERRORTOKEN,\n",
    "        quote_special(TokenKind.NEWLINE.name):\n",
    "            tokenize.NEWLINE,\n",
    "        token_from_token_type(tokenize.NL):\n",
    "            tokenize.NL,\n",
    "    }\n",
    "    # Adding the end-of-string anchor \\Z below, since re.fullmatch wasn't\n",
    "    # available in Python2.\n",
    "    # pytype: disable=module-attr\n",
    "    _NUMBERS = re.compile('(' + tokenize.Number + r')\\Z')\n",
    "    # pytype: disable=module-attr\n",
    "    _SINGLE_STRINGS = re.compile('(' + tokenize.String + r')\\Z')\n",
    "    _TRIPLE_STRING_BEGINNINGS = re.compile(\n",
    "        tokenize.Triple)  # pytype: disable=module-attr\n",
    "    # pytype: disable=module-attr\n",
    "    _COMMENTS = re.compile('(' + tokenize.Comment + r')\\Z')\n",
    "\n",
    "    _EXACT_TOKEN_TYPES = tokenize.EXACT_TOKEN_TYPES.keys()  # pytype: disable=module-attr\n",
    "\n",
    "    # Token types that CubertTokenizer will tokenize by their type and not\n",
    "    # content.\n",
    "    _TOKEN_TYPES_TO_TOKENIZE_BY_TYPE = [\n",
    "        tokenize.NEWLINE, tokenize.DEDENT, tokenize.NL\n",
    "    ]\n",
    "\n",
    "    def tokenize_and_abstract(\n",
    "            self,\n",
    "            source_code):\n",
    "        \"\"\"Produces a language-agnostic tokenization of the input code.\"\"\"\n",
    "        agnostic_tokens: List[AbstractToken] = []\n",
    "\n",
    "        try:\n",
    "            token_tuples = code_to_tokens(source_code)\n",
    "        except (tokenize.TokenError, IndentationError) as e:\n",
    "            logging.warning('The tokenizer raised exception `%s` while parsing %s', e,\n",
    "                            source_code)\n",
    "\n",
    "            # We don't try to do recovery from errors quite yet. Emit just an\n",
    "            # error and end-of-sequence and return.\n",
    "            agnostic_tokens.append(\n",
    "                AbstractToken(\n",
    "                    quote_special(\n",
    "                        TokenKind.ERROR.name),\n",
    "                    TokenKind.ERROR,\n",
    "                    TokenMetadata(\n",
    "                        start=Position(\n",
    "                            line=0, column=0),\n",
    "                        end=Position(\n",
    "                            line=0, column=0))))\n",
    "            agnostic_tokens.append(\n",
    "                AbstractToken(\n",
    "                    quote_special(\n",
    "                        TokenKind.EOS.name),\n",
    "                    TokenKind.EOS,\n",
    "                    TokenMetadata(\n",
    "                        start=Position(\n",
    "                            line=0, column=0),\n",
    "                        end=Position(\n",
    "                            line=0, column=0))))\n",
    "            return agnostic_tokens\n",
    "\n",
    "        for token_tuple in token_tuples:\n",
    "            spelling = token_tuple.string\n",
    "            kind = token_tuple.type\n",
    "\n",
    "            # We'll adjust the spelling of some tokens, e.g., those that we\n",
    "            # tokenize by their type rather than their original spelling. Indentation\n",
    "            # and dedentation tokens are like that.\n",
    "            adjusted_spelling = spelling\n",
    "            token_kind = TokenKind.NONE\n",
    "            if kind == tokenize.NAME:\n",
    "                # Disambiguate identifiers from keywords.\n",
    "                if keyword.iskeyword(spelling):\n",
    "                    token_kind = TokenKind.KEYWORD\n",
    "                else:\n",
    "                    token_kind = TokenKind.IDENTIFIER\n",
    "            else:\n",
    "                if kind in PythonTokenizer._TOKEN_TYPES_TO_TOKENIZE_BY_TYPE:\n",
    "                    # Replace spelling with type.\n",
    "                    adjusted_spelling = token_from_token_type(\n",
    "                        kind)\n",
    "                elif kind is tokenize.INDENT:\n",
    "                    # For INDENT, in particular, we also record the actual spelling too.\n",
    "                    adjusted_spelling = '{indent}{spelling}'.format(\n",
    "                        indent=token_from_token_type(kind),\n",
    "                        spelling=spelling)\n",
    "                elif kind == tokenize.ENDMARKER:\n",
    "                    adjusted_spelling = quote_special(\n",
    "                        TokenKind.EOS.name)\n",
    "\n",
    "                # Map everything according to table.\n",
    "                try:\n",
    "                    token_kind = PythonTokenizer._TOKEN_TYPE_MAP[kind]\n",
    "                except KeyError as ke:\n",
    "                    # It's possible we're here because of async/await. Those kept being\n",
    "                    # turned into keywords and then removed from keywords, so we can't\n",
    "                    # rely on knowing which they are. We'll check by spelling.\n",
    "                    # See: https://bugs.python.org/issue30406\n",
    "                    # and https://bugs.python.org/issue33260\n",
    "                    # and https://bugs.python.org/issue35975\n",
    "                    if spelling in ('async', 'await'):\n",
    "                        token_kind = TokenKind.KEYWORD\n",
    "                    else:\n",
    "                        raise ValueError('While trying to turn Python token %r into an '\n",
    "                                         'agnostic one, raised %r.' %\n",
    "                                         ((spelling, kind), ke))\n",
    "\n",
    "            start_line, start_column = token_tuple.start\n",
    "            end_line, end_column = token_tuple.end\n",
    "            # Unlike other languages, NEWLINE tokens are reported as ending on the\n",
    "            # same line as where they started. We adjust that here, to stick to the\n",
    "            # same convention as other tokenizers.\n",
    "            if ((token_kind == TokenKind.NEWLINE) or\n",
    "                    (kind == tokenize.NL)):\n",
    "                end_line = start_line + 1\n",
    "                end_column = 0\n",
    "\n",
    "            agnostic_tokens.append(\n",
    "                AbstractToken(\n",
    "                    spelling=adjusted_spelling, kind=token_kind,\n",
    "                    metadata=TokenMetadata(\n",
    "                        # Python's tokenizer counts lines starting from 1, so we\n",
    "                        # have to offset what we read from the `TokenInfo` tuple.\n",
    "                        start=Position(\n",
    "                            line=start_line - 1, column=start_column),\n",
    "                        end=Position(\n",
    "                            line=end_line - 1, column=end_column))))\n",
    "\n",
    "        return agnostic_tokens\n",
    "\n",
    "    def untokenize_abstract(self, whole_tokens):\n",
    "        # Reconstruct Python tokenizer tuples, so that Python's untokenize can be\n",
    "        # invoked.\n",
    "        token_tuples: List[Tuple[int, str]] = []\n",
    "\n",
    "        for whole_token in whole_tokens:\n",
    "            if whole_token in PythonTokenizer._EXACT_TOKEN_TYPES:\n",
    "                token_tuples.append((tokenize.OP, whole_token))\n",
    "            elif token_from_token_type(\n",
    "                    tokenize.INDENT) in whole_token:\n",
    "                # We baked the type and spelling into one token. Break them up.\n",
    "                spelling = whole_token.replace(\n",
    "                    token_from_token_type(tokenize.INDENT), '')\n",
    "                token_tuples.append((tokenize.INDENT, spelling))\n",
    "            elif whole_token in PythonTokenizer._REVERSE_TOKEN_MAP:\n",
    "                python_kind = PythonTokenizer._REVERSE_TOKEN_MAP[whole_token]\n",
    "                if python_kind in (tokenize.DEDENT, tokenize.ENDMARKER,\n",
    "                                   tokenize.ERRORTOKEN):\n",
    "                    spelling = ''\n",
    "                else:  # python_kind in (tokenize.NEWLINE, tokenize.NL)\n",
    "                    spelling = '\\n'\n",
    "                token_tuples.append((python_kind, spelling))\n",
    "            elif keyword.iskeyword(whole_token):\n",
    "                token_tuples.append((tokenize.NAME, whole_token))\n",
    "            elif PythonTokenizer._NUMBERS.match(whole_token):\n",
    "                token_tuples.append((tokenize.NUMBER, whole_token))\n",
    "            elif PythonTokenizer._SINGLE_STRINGS.match(whole_token):\n",
    "                token_tuples.append((tokenize.STRING, whole_token))\n",
    "            elif PythonTokenizer._TRIPLE_STRING_BEGINNINGS.match(whole_token):\n",
    "                token_tuples.append((tokenize.STRING, whole_token))\n",
    "            elif PythonTokenizer._COMMENTS.match(whole_token):\n",
    "                token_tuples.append((tokenize.COMMENT, whole_token))\n",
    "            else:\n",
    "                # Everything else we map back to NAME.\n",
    "                token_tuples.append((tokenize.NAME, whole_token))\n",
    "\n",
    "        reconstructed = tokenize.untokenize(typing.cast(Any, token_tuples))\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Thanks to DNGRos for this huggingface/transformers compatible version\n",
    "# https://github.com/google-research/google-research/issues/582\n",
    "#\n",
    "import os\n",
    "import collections\n",
    "from typing import *\n",
    "from transformers import BertTokenizer\n",
    "# from tensor2tensor.data_generators import text_encoder\n",
    "from unif import t2t_text_encoder as text_encoder\n",
    "\n",
    "\n",
    "def combine_tokenizer_with_subword(\n",
    "    initial_tokenizer: CuBertTokenizer,\n",
    "    subword_tokenizer: text_encoder.SubwordTextEncoder,\n",
    ") -> Callable[[str], List[str]]:\n",
    "    # Try to match the functionality at \n",
    "    # https://github.com/google-research/google-research/blob/50c6cd94b5/cubert/code_to_subtokenized_sentences.py#L111-L118\n",
    "    \n",
    "    def tokenize(string: str) -> List[str]:\n",
    "        toks = initial_tokenizer.tokenize(string)\n",
    "        tokens = flatten_list(\n",
    "            subword_tokenizer.decode_list(\n",
    "                subword_tokenizer.encode_without_tokenizing(token)\n",
    "            )\n",
    "            for token in toks\n",
    "        )\n",
    "        return tokens\n",
    "    return tokenize\n",
    "\n",
    "\n",
    "def flatten_list(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "\n",
    "class CuBertHugTokenizer(BertTokenizer):\n",
    "    # A hacky version that seems to work at least for python\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file: str,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            vocab_file=vocab_file,\n",
    "            do_lower_case=False,\n",
    "            do_basic_tokenize=True,\n",
    "            unk_token=\"[UNK]_\",\n",
    "            sep_token=\"[SEP]_\",\n",
    "            pad_token=\"<pad>_\",\n",
    "            cls_token=\"[CLS]_\",\n",
    "            mask_token=\"[MASK]_\",\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    vocab_file)\n",
    "            )\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.first_tokenizer = PythonTokenizer()\n",
    "        self.subword_tokenizer = text_encoder.SubwordTextEncoder(str(vocab_file))\n",
    "        self._combined_func = combine_tokenizer_with_subword(\n",
    "            self.first_tokenizer, self.subword_tokenizer)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return super().__call__(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation='longest_first',\n",
    "            max_length=MAX_SEQUENCE_LENGTH\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def do_lower_case(self):\n",
    "        return False\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self._combined_func(text)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.subword_tokenizer._subtoken_string_to_id[token]\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "        vocab = collections.OrderedDict()\n",
    "        with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "            tokens = reader.readlines()\n",
    "        for index, token in enumerate(tokens):\n",
    "            token = token.rstrip(\"\\n\")\n",
    "            vocab[token] = index\n",
    "        return vocab"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGrPD7CUSEOo",
    "outputId": "6bc0a1b8-9ddf-4800-e274-003e225a4514"
   },
   "source": [
    "!wget -c vocab.txt https://github.com/franpena-kth/learning-deep-learning/raw/master/vocabulary/python_vocabulary.txt"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-22 08:54:53--  http://vocab.txt/\r\n",
      "Resolving vocab.txt (vocab.txt)... failed: nodename nor servname provided, or not known.\r\n",
      "wget: unable to resolve host address ‘vocab.txt’\r\n",
      "--2021-04-22 08:54:53--  https://github.com/franpena-kth/learning-deep-learning/raw/master/vocabulary/python_vocabulary.txt\r\n",
      "Resolving github.com (github.com)... 140.82.121.4\r\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://raw.githubusercontent.com/franpena-kth/learning-deep-learning/master/vocabulary/python_vocabulary.txt [following]\r\n",
      "--2021-04-22 08:54:54--  https://raw.githubusercontent.com/franpena-kth/learning-deep-learning/master/vocabulary/python_vocabulary.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 416 Range Not Satisfiable\r\n",
      "\r\n",
      "    The file is already fully retrieved; nothing to do.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F2LUigzI43fQ"
   },
   "source": [
    "MODEL_VOCAB = './python_vocabulary.txt'\n",
    "code_tokenizer = CuBertHugTokenizer(MODEL_VOCAB)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdmOa3D9Sz8u",
    "outputId": "ce86619d-9879-4d05-8fc6-e273a3648278"
   },
   "source": [
    "# tokenizer = CuBertHugTokenizer(MODEL_VOCAB)\n",
    "import torch\n",
    "\n",
    "print(code_snippets[0])\n",
    "print(code_tokenizer(code_snippets[0]))\n",
    "print(code_tokenizer.tokenize(code_snippets[0]))\n",
    "\n",
    "# print()\n",
    "# print(code_snippets[0])\n",
    "# print(code_tokenizer(code_snippets[0]))\n",
    "# print(code_tokenizer.tokenize(code_snippets[0]))\n",
    "\n",
    "print()\n",
    "print(code_snippets[10])\n",
    "print(code_tokenizer(code_snippets[10]))\n",
    "print(code_tokenizer.tokenize(code_snippets[10]))\n",
    "\n",
    "input_ids = torch.tensor(code_tokenizer(code_snippets[0])['input_ids'], dtype=torch.int)\n",
    "input_ids = torch.reshape(input_ids, (1, -1))\n",
    "print('Code token IDs', input_ids.shape)\n",
    "\n",
    "# tokenized_code = code_tokenizer(code_snippets)\n",
    "# print(tokenized_code[0])"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DCSP fp = open(filename, 'rb') DCNL DCSP source = (fp.read() + '\\n') DCNL DCSP try: DCNL DCSP  DCSP co = compile(source, filename, 'exec') DCNL DCSP except SyntaxError: DCNL DCSP  DCSP print >>sys.stderr, '>>Syntax DCSP error DCSP in', filename DCNL DCSP  DCSP raise DCNL DCSP fp.close() DCNL DCSP return co\n",
      "{'input_ids': [2, 34, 26, 49988, 26, 1882, 24, 394, 20, 373, 16, 22, 2774, 23, 19, 49989, 26, 49988, 26, 777, 24, 20, 1882, 21, 604, 20, 19, 65, 143, 64, 23, 19, 49989, 26, 49988, 26, 219, 25, 49989, 26, 49988, 26, 49988, 26, 8047, 24, 1337, 20, 777, 16, 373, 16, 22, 4214, 23, 19, 49989, 26, 49988, 26, 217, 4368, 149, 25, 49989, 26, 49988, 26, 49988, 26, 129, 2251, 224, 21, 1237, 16, 26865, 37, 4368, 17, 49988, 37, 17, 463, 17, 49988, 37, 17, 115, 23, 16, 373, 49989, 26, 49988, 26, 49988, 26, 244, 49989, 26, 49988, 26, 1882, 21, 587, 20, 19, 49989, 26, 49988, 26, 60, 8047, 7, 121, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "['\\\\u\\\\u\\\\uINDENT\\\\u\\\\u\\\\u ', '_', 'DCSP', '_', 'fp_', '=_', 'open_', '(_', 'filename_', ',_', \"'^_\", 'rb^_', \"'_\", ')_', 'DCNL', '_', 'DCSP', '_', 'source_', '=_', '(_', 'fp_', '._', 'read_', '(_', ')_', '+_', \"'\\\\\\\\^_\", 'n^_', \"'_\", ')_', 'DCNL', '_', 'DCSP', '_', 'try_', ':_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'co_', '=_', 'compile_', '(_', 'source_', ',_', 'filename_', ',_', \"'^_\", 'exec^_', \"'_\", ')_', 'DCNL', '_', 'DCSP', '_', 'except_', 'Syntax^_', 'Error_', ':_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'print_', '>>_', 'sys_', '._', 'stderr_', ',_', \"'>>\", '^_', 'Syntax^_', ' ^_', 'DCSP', '^_', ' ^_', 'error^_', ' ^_', 'DCSP', '^_', ' ^_', 'in^_', \"'_\", ',_', 'filename_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'raise_', 'DCNL', '_', 'DCSP', '_', 'fp_', '._', 'close_', '(_', ')_', 'DCNL', '_', 'DCSP', '_', 'return_', 'co_', '\\\\u\\\\u\\\\uDEDENT\\\\u\\\\u\\\\u_', '\\\\u\\\\u\\\\uEOS\\\\u\\\\u\\\\u_']\n",
      "\n",
      " DCSP for body_charset in ('US-ASCII', 'ISO-8859-1', 'UTF-8'): DCNL DCSP  DCSP try: DCNL DCSP  DCSP  DCSP BODY.encode(body_charset) DCNL DCSP  DCSP except UnicodeError: DCNL DCSP  DCSP  DCSP pass DCNL DCSP  DCSP else: DCNL DCSP  DCSP  DCSP break DCNL DCSP msg = MIMEText(BODY.encode(body_charset), 'html', body_charset) DCNL DCSP msg['From'] = SENDER DCNL DCSP msg['To'] = TO DCNL DCSP msg['Subject'] = SUBJECT DCNL DCSP SMTP_PORT = 587 DCNL DCSP session = smtplib.SMTP(SMTP_SERVER, SMTP_PORT) DCNL DCSP session.starttls() DCNL DCSP session.login(FROM, PASSWORD) DCNL DCSP session.sendmail(SENDER, TO, msg.as_string()) DCNL DCSP session.quit()\n",
      "{'input_ids': [2, 34, 26, 49988, 26, 82, 4578, 10976, 68, 20, 22, 1993, 40, 10734, 23, 16, 22, 2877, 40, 1548, 40, 47, 23, 16, 22, 3999, 40, 92, 23, 19, 25, 49989, 26, 49988, 26, 49988, 26, 219, 25, 49989, 26, 49988, 26, 49988, 26, 49988, 26, 21379, 26, 21, 1250, 20, 4578, 10976, 19, 49989, 26, 49988, 26, 49988, 26, 217, 2920, 149, 25, 49989, 26, 49988, 26, 49988, 26, 49988, 26, 346, 49989, 26, 49988, 26, 49988, 26, 120, 25, 49989, 26, 49988, 26, 49988, 26, 49988, 26, 873, 49989, 26, 49988, 26, 377, 24, 10910, 631, 20, 21379, 26, 21, 1250, 20, 4578, 10976, 19, 16, 22, 459, 23, 16, 4578, 10976, 19, 49989, 26, 49988, 26, 377, 31, 22, 849, 23, 30, 24, 14263, 7071, 49989, 26, 49988, 26, 377, 31, 22, 400, 23, 30, 24, 20934, 49989, 26, 49988, 26, 377, 31, 22, 10476, 23, 30, 24, 34881, 26, 49989, 26, 49988, 26, 40045, 27, 5058, 24, 25021, 49989, 26, 49988, 26, 500, 24, 36853, 21, 40045, 26, 20, 40045, 27, 21279, 16, 40045, 27, 5058, 19, 49989, 26, 49988, 26, 500, 21, 19814, 3061, 20, 19, 49989, 26, 49988, 26, 500, 21, 2333, 20, 36990, 1461, 16, 12051, 19, 49989, 26, 49988, 26, 500, 21, 48999, 7493, 20, 14263, 7071, 16, 20934, 16, 377, 21, 1166, 416, 20, 19, 19, 49989, 26, 49988, 26, 500, 21, 7059, 20, 19, 7, 121, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "['\\\\u\\\\u\\\\uINDENT\\\\u\\\\u\\\\u ', '_', 'DCSP', '_', 'for_', 'body\\\\u^_', 'charset_', 'in_', '(_', \"'^_\", 'US^_', '-^_', 'ASCII^_', \"'_\", ',_', \"'^_\", 'ISO^_', '-^_', '8859^_', '-^_', '1^_', \"'_\", ',_', \"'^_\", 'UTF^_', '-^_', '8^_', \"'_\", ')_', ':_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'try_', ':_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'DCSP', '_', 'BODY', '_', '._', 'encode_', '(_', 'body\\\\u^_', 'charset_', ')_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'except_', 'Unicode^_', 'Error_', ':_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'DCSP', '_', 'pass_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'else_', ':_', 'DCNL', '_', 'DCSP', '_', 'DCSP', '_', 'DCSP', '_', 'break_', 'DCNL', '_', 'DCSP', '_', 'msg_', '=_', 'MIME^_', 'Text_', '(_', 'BODY', '_', '._', 'encode_', '(_', 'body\\\\u^_', 'charset_', ')_', ',_', \"'^_\", 'html^_', \"'_\", ',_', 'body\\\\u^_', 'charset_', ')_', 'DCNL', '_', 'DCSP', '_', 'msg_', '[_', \"'^_\", 'From^_', \"'_\", ']_', '=_', 'SEND', 'ER_', 'DCNL', '_', 'DCSP', '_', 'msg_', '[_', \"'^_\", 'To^_', \"'_\", ']_', '=_', 'TO_', 'DCNL', '_', 'DCSP', '_', 'msg_', '[_', \"'^_\", 'Subject^_', \"'_\", ']_', '=_', 'SUBJECT', '_', 'DCNL', '_', 'DCSP', '_', 'SMTP', '\\\\u^_', 'PORT_', '=_', '587_', 'DCNL', '_', 'DCSP', '_', 'session_', '=_', 'smtplib_', '._', 'SMTP', '_', '(_', 'SMTP', '\\\\u^_', 'SERVER_', ',_', 'SMTP', '\\\\u^_', 'PORT_', ')_', 'DCNL', '_', 'DCSP', '_', 'session_', '._', 'startt', 'ls_', '(_', ')_', 'DCNL', '_', 'DCSP', '_', 'session_', '._', 'login_', '(_', 'FRO', 'M_', ',_', 'PASSWORD_', ')_', 'DCNL', '_', 'DCSP', '_', 'session_', '._', 'sendma', 'il_', '(_', 'SEND', 'ER_', ',_', 'TO_', ',_', 'msg_', '._', 'as\\\\u^_', 'string_', '(_', ')_', ')_', 'DCNL', '_', 'DCSP', '_', 'session_', '._', 'quit_', '(_', ')_', '\\\\u\\\\u\\\\uDEDENT\\\\u\\\\u\\\\u_', '\\\\u\\\\u\\\\uEOS\\\\u\\\\u\\\\u_']\n",
      "Code token IDs torch.Size([1, 512])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nlAobQihBHw"
   },
   "source": [
    "## Tokenize natural language"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEqCZ0qlUQDU",
    "outputId": "202f17e4-2a83-4ace-d4e3-d09e28d9262c"
   },
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = 'I love Paris'\n",
    "\n",
    "tokenization_results = bert_tokenizer(sentence, padding='max_length', add_special_tokens=True, max_length=10, return_tensors='pt')\n",
    "print('Tokenization results', tokenization_results)\n",
    "\n",
    "hidden_rep, cls_head = bert_model(tokenization_results['input_ids'], attention_mask=tokenization_results['attention_mask']).values()\n",
    "print('Token ids', tokenization_results['input_ids'].shape)\n",
    "print('Hidden rep', hidden_rep.shape)\n",
    "print('CLS head', cls_head.shape)\n",
    "# print(hidden_rep)\n",
    "\n",
    "# https://medium.com/@noa.kel/using-bert-with-pytorch-b9624edcda4e"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization results {'input_ids': tensor([[ 101, 1045, 2293, 3000,  102,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
      "Token ids torch.Size([1, 10])\n",
      "Hidden rep torch.Size([1, 10, 768])\n",
      "CLS head torch.Size([1, 768])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoshHbyeR5ad"
   },
   "source": [
    "## Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dsCHgfgGR9k7"
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CodeDescDataset(Dataset):\n",
    "\n",
    "    def __init__(self, code_snippets_file, descriptions_file):\n",
    "        self.code_tokenizer = CuBertHugTokenizer(MODEL_VOCAB)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.code_vocab_size = self.code_tokenizer.vocab_size\n",
    "        self.desc_vocab_size = self.desc_tokenizer.vocab_size\n",
    "        # Load data here\n",
    "        # code_snippets_file = './data/parallel_bodies'\n",
    "        with open(code_snippets_file) as f:\n",
    "            self.code_snippets = [line.rstrip() for line in f][:DATASET_SIZE]\n",
    "\n",
    "        # descriptions_file = './data/parallel_desc'\n",
    "        with open(descriptions_file, encoding=\"ISO-8859-1\") as f:\n",
    "            self.descriptions = [line.rstrip() for line in f][:DATASET_SIZE]\n",
    "        \n",
    "        assert len(self.code_snippets) == len(self.descriptions), 'The code snippets file must have the same size as the descriptions file'\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return dataset size\n",
    "        return len(self.code_snippets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load the code and the descriptions\n",
    "        code_snippet = self.code_snippets[index]\n",
    "        description = self.descriptions[index]\n",
    "\n",
    "        # Return a (code_token_ids, desc_token_ids) tuple\n",
    "        tokenized_code = self.code_tokenizer(code_snippet)\n",
    "        tokenized_desc = self.desc_tokenizer(\n",
    "            description, padding='max_length', add_special_tokens=True,\n",
    "            max_length=MAX_SEQUENCE_LENGTH, return_tensors='pt',\n",
    "            truncation=True)\n",
    "        \n",
    "        return tokenized_code, tokenized_desc\n"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYaoK9DeWTMx",
    "outputId": "1a8ca2e9-4e62-4bb4-c166-b8fd7acf9114"
   },
   "source": [
    "code_snippets_file = './data/parallel_bodies'\n",
    "descriptions_file = './data/parallel_desc'\n",
    "\n",
    "dataset = CodeDescDataset(code_snippets_file, descriptions_file)\n",
    "code_snippet, description = dataset[0]\n",
    "print()\n",
    "code_snippet, description = dataset[10]\n",
    "print()\n",
    "code_snippet, description = dataset[100]\n",
    "\n",
    "# print('Code snippet', code_snippet)\n",
    "# print('Description', description)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta1QX2oVj4oa"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4w6NLJVShHKO"
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "class OldUNIF(nn.Module):\n",
    "\n",
    "    def __init__(self, code_vocab_size, desc_vocab_size, embedding_size):\n",
    "        super(UNIF, self).__init__()\n",
    "        self.code_embedding_layer = nn.Embedding(num_embeddings=code_vocab_size, embedding_dim=embedding_size)\n",
    "        self.desc_embedding_layer = nn.Embedding(num_embeddings=desc_vocab_size, embedding_dim=embedding_size)\n",
    "        attention_weights = torch.nn.init.uniform_(torch.empty(embedding_size, embedding_size, dtype=torch.float32, requires_grad=True))\n",
    "        self.attention_weights = nn.parameter.Parameter(attention_weights, requires_grad=True)\n",
    "        # self.cosine_layer = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, code_token_ids, desc_token_ids):\n",
    "\n",
    "        # print('Max code', torch.max(code_token_ids))\n",
    "        # print('Max desc', torch.max(desc_token_ids))\n",
    "\n",
    "        # print('Code embedding layer', self.code_embedding_layer, self.code_embedding_layer.num_embeddings, self.code_embedding_layer.embedding_dim)\n",
    "\n",
    "        # print('Code tokens IDs', code_token_ids.shape)\n",
    "        # print('Desc tokens IDs', desc_token_ids.shape)\n",
    "\n",
    "        code_embedding = self.code_embedding_layer(code_token_ids)\n",
    "        desc_embedding = self.desc_embedding_layer(desc_token_ids)\n",
    "\n",
    "        # Calculate the attention weights for the code embedding layer\n",
    "        # code_embedding = self.attention_weights(code_embedding)\n",
    "        # print('Attention weights pre', self.attention_weights.shape)\n",
    "        # print('Code embedding', code_embedding.shape)\n",
    "        batch_size = code_token_ids.shape[0]\n",
    "        attention_weights = self.attention_weights.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        # my_tensor.unsqueeze(0).repeat(7, 1, 1)\n",
    "        # print('Attention weights post', attention_weights.shape)\n",
    "        # code_embedding = functional.softmax(torch.bmm(attention_weights, code_embedding))\n",
    "        code_embedding = torch.bmm(code_embedding, attention_weights)\n",
    "        # code_embedding = torch.bmm(attention_weights, code_embedding)\n",
    "        code_embedding = torch.sum(code_embedding, 1)\n",
    "            \n",
    "\n",
    "        # Calculate the average of the embeddings for the desc embedding layer\n",
    "        desc_embedding = torch.mean(desc_embedding, 1)\n",
    "\n",
    "        # output = self.cosine_layer(code_embedding, desc_embedding)\n",
    "\n",
    "        # print('Code embedding', code_embedding.shape)\n",
    "        # print('Desc embedding', desc_embedding.shape)\n",
    "\n",
    "        return code_embedding, desc_embedding\n",
    "    \n",
    "    def encode_code_snippet(self, code_token_ids):\n",
    "        code_embedding = self.code_embedding_layer(code_token_ids)\n",
    "        batch_size = 1\n",
    "        attention_weights = self.attention_weights.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        code_embedding = torch.bmm(code_embedding, attention_weights)\n",
    "\n",
    "        return code_embeddings\n",
    "    \n",
    "    def encode_description(self, code_desc_ids):\n",
    "\n",
    "        desc_embedding = self.desc_embedding_layer(desc_token_ids)\n",
    "        return desc_embedding\n",
    "\n"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u4jyvuBzfXCd"
   },
   "source": [
    "class UNIF(nn.Module):\n",
    "\n",
    "    def __init__(self, code_vocab_size, desc_vocab_size, embedding_size):\n",
    "        super(UNIF, self).__init__()\n",
    "        self.code_embedding_layer = nn.Embedding(num_embeddings=code_vocab_size, embedding_dim=embedding_size)\n",
    "        self.desc_embedding_layer = nn.Embedding(num_embeddings=desc_vocab_size, embedding_dim=embedding_size)\n",
    "        attention_weights = torch.nn.init.uniform_(torch.empty(embedding_size, 1, dtype=torch.float32, requires_grad=True))\n",
    "        self.attention_weights = nn.parameter.Parameter(attention_weights, requires_grad=True)\n",
    "        # self.cosine_layer = nn.CosineSimilarity()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, code_token_ids, desc_token_ids):\n",
    "\n",
    "        # print('Max code', torch.max(code_token_ids))\n",
    "        # print('Max desc', torch.max(desc_token_ids))\n",
    "\n",
    "        # print('Code embedding layer', self.code_embedding_layer, self.code_embedding_layer.num_embeddings, self.code_embedding_layer.embedding_dim)\n",
    "\n",
    "        # print('Code tokens IDs', code_token_ids.shape)\n",
    "        # print('Desc tokens IDs', desc_token_ids.shape)\n",
    "\n",
    "        code_embedding = self.code_embedding_layer(code_token_ids)\n",
    "        desc_embedding = self.desc_embedding_layer(desc_token_ids)\n",
    "\n",
    "        # Calculate the attention weights for the code embedding layer\n",
    "        # code_embedding = self.attention_weights(code_embedding)\n",
    "        # print('Attention weights pre', self.attention_weights.shape)\n",
    "        # print('Code embedding', code_embedding.shape)\n",
    "        batch_size = code_token_ids.shape[0]\n",
    "        attention_weights = self.attention_weights.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        # attention_weights = self.attention_weights.unsqueeze(0).repeat(batch_size, 1).unsqueeze(-1)\n",
    "        # print('Attention weights post', attention_weights.shape)\n",
    "        # code_embedding = functional.softmax(torch.bmm(attention_weights, code_embedding))\n",
    "        # code_embedding = torch.bmm(code_embedding, attention_weights)\n",
    "        # attention_scores = torch.where(mask.view(batch_size, -1) != 0., torch.bmm(context, query).squeeze(-1), self.minus_inf)\n",
    "        attention_scores = torch.bmm(code_embedding, attention_weights).squeeze(-1)\n",
    "        # print('Attention scores', attention_scores.shape)\n",
    "        # code_embedding = torch.bmm(attention_weights, code_embedding)\n",
    "        attention_weights = self.softmax(attention_scores.squeeze(-1))\n",
    "        # print('Attention weights softmax', attention_weights.shape)\n",
    "        \n",
    "        # code_embedding = torch.sum(code_embedding, 1)\n",
    "        code_embedding = (attention_weights.unsqueeze(-1) * code_embedding).sum(1)\n",
    "        # print('Code embedding sum', code_embedding.shape)\n",
    "\n",
    "            \n",
    "\n",
    "        # Calculate the average of the embeddings for the desc embedding layer\n",
    "        desc_embedding = torch.mean(desc_embedding, 1)\n",
    "\n",
    "        # output = self.cosine_layer(code_embedding, desc_embedding)\n",
    "\n",
    "        # print('Code embedding', code_embedding.shape)\n",
    "        # print('Desc embedding', desc_embedding.shape)\n",
    "\n",
    "        return code_embedding, desc_embedding\n",
    "    \n",
    "    def encode_code_snippet(self, code_token_ids):\n",
    "        code_embedding = self.code_embedding_layer(code_token_ids)\n",
    "        batch_size = 1\n",
    "        attention_weights = self.attention_weights.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        code_embedding = torch.bmm(code_embedding, attention_weights)\n",
    "\n",
    "        return code_embeddings\n",
    "    \n",
    "    def encode_description(self, code_desc_ids):\n",
    "\n",
    "        desc_embedding = self.desc_embedding_layer(desc_token_ids)\n",
    "        return desc_embedding"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4ohf-1tgJuS"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "joJI00aU7JT6"
   },
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def train(model, loss_function, optimiser, code_token_ids, desc_token_ids):\n",
    "    model.zero_grad()\n",
    "\n",
    "    code_embedding, desc_embedding = model(code_token_ids, desc_token_ids)\n",
    "\n",
    "    POSITIVE_SIMILARITY = torch.ones(1)\n",
    "    NEGATIVE_SIMILARITY = -torch.ones(1)\n",
    "    loss = loss_function(code_embedding, desc_embedding, POSITIVE_SIMILARITY)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    return code_embedding, desc_embedding, loss.item()\n",
    "\n",
    "\n",
    "def train_cycle():\n",
    "    # n_iters = 100000\n",
    "    # print_every = 5000\n",
    "    # plot_every = 1000\n",
    "    n_iters = DATASET_SIZE\n",
    "    print_every = 500\n",
    "    plot_every = 500\n",
    "    embedding_size = 128\n",
    "\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    code_snippets_file = './data/parallel_bodies'\n",
    "    descriptions_file = './data/parallel_desc'\n",
    "    dataset = CodeDescDataset(code_snippets_file, descriptions_file)\n",
    "    n_hidden = 128\n",
    "    model = UNIF(dataset.code_vocab_size, dataset.desc_vocab_size, embedding_size)\n",
    "\n",
    "    loss_function = nn.CosineEmbeddingLoss()\n",
    "    learning_rate = 0.005  # If you set this too high, it might explode. If too low, it might not learn\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(n_iters):\n",
    "        # print(iter)\n",
    "        tokenized_code, tokenized_desc = dataset[iter]\n",
    "        code_token_ids = torch.tensor(tokenized_code['input_ids'], dtype=torch.int)\n",
    "        code_token_ids = code_token_ids.reshape(1, -1)\n",
    "        desc_token_ids = tokenized_desc['input_ids']\n",
    "        desc_token_ids = desc_token_ids.reshape(1, -1)\n",
    "        code_embedding, desc_embedding, loss = train(model, loss_function, optimiser, code_token_ids, desc_token_ids)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if (iter + 1) % print_every == 0:\n",
    "            print('%d %d%% (%s) %.4f' % (iter + 1, (iter + 1) / n_iters * 100, timeSince(start), current_loss / print_every))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if (iter + 1) % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "\n",
    "    return model, current_loss, all_losses"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ebzArguJGCE"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_GQnnAqEJFDh"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot(all_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(all_losses)\n",
    "    plt.show()"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pLQSyv-WcvME",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "outputId": "cd67bc5d-3a27-4497-a488-b43b20bec613"
   },
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "\n",
    "unif_model, current_loss, all_losses = train_cycle()\n",
    "plot(all_losses)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 5% (0m 11s) 0.9036\n",
      "1000 10% (0m 19s) 0.8806\n",
      "1500 15% (0m 27s) 0.8351\n",
      "2000 20% (0m 36s) 0.8060\n",
      "2500 25% (0m 44s) 0.7886\n",
      "3000 30% (0m 53s) 0.7804\n",
      "3500 35% (1m 2s) 0.7345\n",
      "4000 40% (2m 36s) 0.7112\n",
      "4500 45% (2m 43s) 0.6677\n",
      "5000 50% (2m 51s) 0.6370\n",
      "5500 55% (2m 59s) 0.6180\n",
      "6000 60% (3m 6s) 0.5979\n",
      "6500 65% (3m 14s) 0.5642\n",
      "7000 70% (3m 21s) 0.5434\n",
      "7500 75% (3m 29s) 0.5259\n",
      "8000 80% (3m 37s) 0.5050\n",
      "8500 85% (3m 45s) 0.5252\n",
      "9000 90% (3m 54s) 0.5898\n",
      "9500 95% (4m 1s) 0.4439\n",
      "10000 100% (4m 9s) 0.4047\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IXnZmixjcytB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa67793f-d316-414d-aa0f-acf3f8d27246"
   },
   "source": [
    "# plot(all_losses)\n",
    "# attention_weights = self.attention_weights.repeat(batch_size, 1, 1)\n",
    "\n",
    "my_tensor = torch.tensor(range(10)).reshape(2,5)\n",
    "print(my_tensor.shape)\n",
    "B = my_tensor.unsqueeze(0).repeat(7, 1, 1)\n",
    "print(B.shape)"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([7, 2, 5])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z5W5RucCrEma"
   },
   "source": [
    "# torch.save(model.state_dict(), f'./unif_model_attention.ckpt')"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GVQmPfDtpYB"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g4QbTPgBm2jb"
   },
   "source": [
    "# def evaluate(model, dataX, dataY, scaler, train_size, sequence_length):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         loss_function = nn.MSELoss()\n",
    "#         model.reset_hidden_state(dataX.size(0))\n",
    "#         predicted_test = model(dataX)\n",
    "#         loss = loss_function(predicted_test, dataY)\n",
    "#         print(f'Test set loss: {loss.item():10.10f}')\n",
    "#         plot_series(\n",
    "#             sequence_length, dataX[0, :, 0], dataY[0, 0], predicted_test[0, 0])\n",
    "\n",
    "#         plot_true_vs_predicted(dataY, predicted_test, train_size, scaler)\n",
    "\n",
    "\n",
    "def evaluate(model, loss_function, code_token_ids, desc_token_ids):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        code_embedding, desc_embedding = model(code_token_ids, desc_token_ids)\n",
    "        POSITIVE_SIMILARITY = torch.ones(1)\n",
    "        NEGATIVE_SIMILARITY = -torch.ones(1)\n",
    "        loss = loss_function(code_embedding, desc_embedding, POSITIVE_SIMILARITY)\n",
    "        # print(f'Test set loss: {loss.item():10.10f}')\n",
    "\n",
    "    return code_embedding, desc_embedding, loss.item()\n",
    "\n",
    "\n",
    "def evaluation_cycle(model):\n",
    "    # n_iters = 100000\n",
    "    # print_every = 5000\n",
    "    # plot_every = 1000\n",
    "    n_iters = DATASET_SIZE\n",
    "    print_every = 500\n",
    "    plot_every = 500\n",
    "    embedding_size = 128\n",
    "\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    code_snippets_file = './data/parallel_bodies'\n",
    "    descriptions_file = './data/parallel_desc'\n",
    "    dataset = CodeDescDataset(code_snippets_file, descriptions_file)\n",
    "    n_hidden = 128\n",
    "    model = UNIF(dataset.code_vocab_size, dataset.desc_vocab_size, embedding_size)\n",
    "\n",
    "    loss_function = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    tokenized_code_data = torch.empty(len(dataset), MAX_SEQUENCE_LENGTH, dtype=torch.int)\n",
    "    tokenized_desc_data = torch.empty(len(dataset), MAX_SEQUENCE_LENGTH, dtype=torch.int)\n",
    "\n",
    "    for iter in range(n_iters):\n",
    "        # print(iter)\n",
    "        tokenized_code, tokenized_desc = dataset[iter]\n",
    "        code_token_ids = torch.tensor(tokenized_code['input_ids'], dtype=torch.int)\n",
    "        code_token_ids = code_token_ids.reshape(1, -1)\n",
    "        desc_token_ids = tokenized_desc['input_ids']\n",
    "        desc_token_ids = desc_token_ids.reshape(1, -1)\n",
    "        tokenized_code_data[iter] = code_token_ids\n",
    "        tokenized_desc_data[iter] = desc_token_ids\n",
    "    \n",
    "    print(tokenized_code_data.shape)\n",
    "    print(tokenized_desc_data.shape)\n",
    "    # print(tokenized_code_data[0])\n",
    "    # print(tokenized_code_data[10])\n",
    "    # print(tokenized_desc_data[0])\n",
    "    # print(tokenized_desc_data[10])\n",
    "\n",
    "    # for iter in range(n_iters):\n",
    "    #     # print(iter)\n",
    "    #     tokenized_code, tokenized_desc = dataset[iter]\n",
    "    #     code_token_ids = torch.tensor(tokenized_code['input_ids'], dtype=torch.int)\n",
    "    #     code_token_ids = code_token_ids.reshape(1, -1)\n",
    "    #     desc_token_ids = tokenized_desc['input_ids']\n",
    "    #     desc_token_ids = desc_token_ids.reshape(1, -1)\n",
    "    #     code_embedding, desc_embedding, loss = evaluate(model, loss_function, code_token_ids, desc_token_ids)\n",
    "    #     # print('Code embedding', code_embedding.shape)\n",
    "    #     # print('Desc embedding', desc_embedding.shape)\n",
    "    #     current_loss += loss\n",
    "\n",
    "    #     # Print iter number, loss, name and guess\n",
    "    #     if (iter + 1) % print_every == 0:\n",
    "    #         print('%d %d%% (%s) %.4f' % (iter + 1, (iter + 1) / n_iters * 100, timeSince(start), current_loss / print_every))\n",
    "\n",
    "    #     # Add current loss avg to list of losses\n",
    "    #     if (iter + 1) % plot_every == 0:\n",
    "    #         all_losses.append(current_loss / plot_every)\n",
    "    #         current_loss = 0\n",
    "\n",
    "    return model, current_loss, all_losses"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DDnYA10XdC-u"
   },
   "source": [
    "def tokenize_data(dataset):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    tokenized_code_data = torch.empty(len(dataset), MAX_SEQUENCE_LENGTH, dtype=torch.int)\n",
    "    tokenized_desc_data = torch.empty(len(dataset), MAX_SEQUENCE_LENGTH, dtype=torch.int)\n",
    "\n",
    "    for iter in range(len(dataset)):\n",
    "        # print(iter)\n",
    "        tokenized_code, tokenized_desc = dataset[iter]\n",
    "        code_token_ids = torch.tensor(tokenized_code['input_ids'], dtype=torch.int)\n",
    "        code_token_ids = code_token_ids.reshape(1, -1)\n",
    "        desc_token_ids = tokenized_desc['input_ids']\n",
    "        desc_token_ids = desc_token_ids.reshape(1, -1)\n",
    "        tokenized_code_data[iter] = code_token_ids\n",
    "        tokenized_desc_data[iter] = desc_token_ids\n",
    "    \n",
    "    # print(tokenized_code_data.shape)\n",
    "    # print(tokenized_desc_data.shape)\n",
    "\n",
    "    finish = time.time()\n",
    "    total_time = finish - start\n",
    "\n",
    "    print('Time: ', total_time)\n",
    "\n",
    "    return tokenized_code_data, tokenized_desc_data"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Mvcw103a9xi",
    "outputId": "8c1f368d-bea9-4492-997c-eb9fc75116f0"
   },
   "source": [
    "code_snippets_file = './data/parallel_bodies'\n",
    "descriptions_file = './data/parallel_desc'\n",
    "dataset = CodeDescDataset(code_snippets_file, descriptions_file)\n",
    "\n",
    "tokenized_code_data, tokenized_desc_data = tokenize_data(dataset)\n",
    "print('Tokenized code data', tokenized_code_data.shape)\n",
    "print('Tokenized desc data', tokenized_desc_data.shape)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  119.98514676094055\n",
      "Tokenized code data torch.Size([10000, 512])\n",
      "Tokenized desc data torch.Size([10000, 512])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHhDGDHXgmGN",
    "outputId": "6fcae5a9-0871-4902-8ded-dc9ae6bbcb10"
   },
   "source": [
    "code_embedding_data, desc_embedding_data = unif_model(tokenized_code_data, tokenized_desc_data)\n",
    "print('Code embedding data', code_embedding_data.shape)\n",
    "print('Desc embedding data', desc_embedding_data.shape)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code embedding data torch.Size([10000, 128])\n",
      "Desc embedding data torch.Size([10000, 128])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zRNCOJoMxoFE"
   },
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "\n",
    "# evaluation_cycle(model)\n",
    "# model, current_loss, all_losses = evaluation_cycle()\n",
    "# plot(all_losses)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijPWiV8fwo5d",
    "outputId": "75ce63a0-fffc-42ec-b212-a94e3fb71836"
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy\n",
    "\n",
    "\n",
    "def get_top_n(n, results):\n",
    "    count = 0\n",
    "    for r in results:\n",
    "        if results[r] < n:\n",
    "            count+= 1\n",
    "    return count / len(results)\n",
    "\n",
    "\n",
    "def my_test(code_embeddings, desc_embeddings):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        results = {}\n",
    "\n",
    "        for rowid, desc_embedding in enumerate(desc_embeddings):\n",
    "            # Calculate the cosine similarity between the code and desc embeddings\n",
    "            code_desc_similarity = cosine_similarity(code_embeddings[rowid].reshape((1, -1)), desc_embedding.reshape((1, -1)))\n",
    "\n",
    "            other_code_embeddings = numpy.delete(code_embeddings, rowid, 0)\n",
    "            tiled_desc = torch.Tensor(numpy.tile(desc_embedding, (other_code_embeddings.shape[0], 1)))\n",
    "\n",
    "            # print('Other + tiled', other_code_embeddings.shape, tiled_desc.shape)\n",
    "\n",
    "            # Calculate the cosine similarity between the description vector and all the code snippets excepting the code that matches the desc\n",
    "            ress = cosine_similarity(other_code_embeddings, tiled_desc)\n",
    "            results[rowid] = len(ress[ress > code_desc_similarity])\n",
    "        \n",
    "        top_1 = get_top_n(1, results)\n",
    "        top_3 = get_top_n(3, results)\n",
    "        top_5 = get_top_n(5, results)\n",
    "\n",
    "        print('Top 1', top_1)\n",
    "        print('Top 3', top_3)\n",
    "        print('Top 5', top_5)\n",
    "\n",
    "\n",
    "code_embeddings = [\n",
    "    [1.0, 3.0],\n",
    "    [1.1, 3.0],\n",
    "    [2.0, -1.0],\n",
    "]\n",
    "\n",
    "desc_embeddings = [\n",
    "    [1.1, 3.0],\n",
    "    [4.0, 5.0],\n",
    "    [1.0, 3.0],\n",
    "]\n",
    "\n",
    "code_embeddings = torch.Tensor(code_embeddings)\n",
    "desc_embeddings = torch.Tensor(desc_embeddings)\n",
    "\n",
    "print(code_embeddings.shape)\n",
    "print(desc_embeddings.shape)\n",
    "\n",
    "my_test(code_embeddings, desc_embeddings)\n",
    "\n",
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)\n",
    "output = cos(code_embeddings, desc_embeddings)\n",
    "\n",
    "print(input1.shape, input2.shape, output.shape)\n"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "Top 1 0.3333333333333333\n",
      "Top 3 1.0\n",
      "Top 5 1.0\n",
      "torch.Size([100, 128]) torch.Size([100, 128]) torch.Size([3])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "viaTXbQ9g9BL"
   },
   "source": [
    "# def get_top_n(n, results):\n",
    "#     count = 0\n",
    "#     for r in results:\n",
    "#         if results[r] < n:\n",
    "#             count+= 1\n",
    "#     return count / len(results)\n",
    "\n",
    "\n",
    "# # def test(code_embeddings, desc_embeddings):\n",
    "# def test(data_path, cos_model, results_path, code_length, desc_length, batch_id):\n",
    "#     test_tokens = load_hdf5(data_path + \"test.tokens.h5\" , 0, 10000)\n",
    "#     test_desc = load_hdf5(data_path + \"test.desc.h5\" , 0, 10000)\n",
    "\n",
    "#     test_tokens = pad(test_tokens, code_length)\n",
    "#     test_desc = pad(test_desc, desc_length)\n",
    "\n",
    "#     results = {}\n",
    "#     progress_bar = tqdm(total=len(test_desc))\n",
    "\n",
    "#     for rowid, desc in enumerate(test_desc):\n",
    "\n",
    "#         # Calculate the cosine similarity between the code and desc embeddings\n",
    "#         expected_best_result = cos_model.predict([test_tokens[rowid].reshape((1, -1)), test_desc[rowid].reshape((1, -1))])[0][0]\n",
    "#         deleted_tokens = np.delete(test_tokens, rowid, 0)\n",
    "#         tiled_desc = np.tile(desc, (deleted_tokens.shape[0], 1))\n",
    "\n",
    "#         # Calculate the cosine similarity between the description vector and all the code snippets excepting the code that matches the desc\n",
    "#         ress = cos_model.predict([deleted_tokens, tiled_desc], batch_size=32*4)\n",
    "#         results[rowid] = len(ress[ress > expected_best_result])\n",
    "#         progress_bar.update(1)\n",
    "\n",
    "#     progress_bar.close()\n",
    "\n",
    "#     top_1 = get_top_n(1, results)\n",
    "#     top_3 = get_top_n(3, results)\n",
    "#     top_5 = get_top_n(5, results)\n",
    "\n",
    "#     print(top_1)\n",
    "#     print(top_3)\n",
    "#     print(top_5)\n",
    "\n",
    "#     name = results_path+\"/results-unif-dcs-\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\"\n",
    "\n",
    "#     f = open(name, \"a\")\n",
    "\n",
    "#     f.write(\"batch,top1,top3,top5\\n\")\n",
    "#     f.write(str(batch_id)+\",\"+str(top_1) + \",\" + str(top_3) + \",\" + str(top_5) + \"\\n\")\n",
    "#     f.close()\n",
    "\n",
    "\n"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPO4RPOakLtz",
    "outputId": "2fbc46c8-b707-472a-ff0c-b8fbe55bc699"
   },
   "source": [
    "my_test(code_embedding_data, desc_embedding_data)"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 0.0001\n",
      "Top 3 0.0003\n",
      "Top 5 0.0004\n"
     ]
    }
   ]
  }
 ]
}